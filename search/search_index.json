{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KSML Documentation Welcome to the KSML documentation, Use the menu on the left to navigate through the various sections Testing a change Adding a new paragraph to verify the pages deployment flow. Quick Start If you want to get going quickly, go to the KSML Quickstart. Introduction KSML allows anyone to specify a powerful Kafka Streams application in just a few lines of YAML and Python snippets. Contents Introduction Stream Types Functions Pipelines Operations Data Types Runners Language specification Getting Started Release Notes","title":"Home"},{"location":"#ksml-documentation","text":"Welcome to the KSML documentation, Use the menu on the left to navigate through the various sections","title":"KSML Documentation"},{"location":"#testing-a-change","text":"Adding a new paragraph to verify the pages deployment flow.","title":"Testing a change"},{"location":"#quick-start","text":"If you want to get going quickly, go to the KSML Quickstart.","title":"Quick Start"},{"location":"#introduction","text":"KSML allows anyone to specify a powerful Kafka Streams application in just a few lines of YAML and Python snippets.","title":"Introduction"},{"location":"#contents","text":"Introduction Stream Types Functions Pipelines Operations Data Types Runners Language specification Getting Started Release Notes","title":"Contents"},{"location":"introduction/","text":"KSML: Kafka Streams for Low Code Environments Abstract Kafka Streams has captured the hearts and minds of many developers that want to develop streaming applications on top of Kafka. But as powerful as the framework is, Kafka Streams has had a hard time getting around the requirement of writing Java code and setting up build pipelines. There were some attempts to rebuild Kafka Streams, but up until now popular languages like Python did not receive equally powerful (and maintained) stream processing frameworks. In this article we will present a new declarative approach to unlock Kafka Streams, called KSML. By the time you finish reading this document, you will be able to write streaming applications yourself, using only a few simple basic rules and Python snippets. Setting up a test environment KSML in practice Example 1. Inspect data on a topic Example 2. Copying data to another topic Example 3. Filtering data Example 4. Branching messages Example 5. Dynamic routing Example 6. Multiple pipelines Setting up a test environment To demonstrate KSML's capabilities, you will need a working Kafka cluster, or an Axual Platform/Cloud environment. Check out the Runners page to configure KSML. We set up a test topic, called ksml_sensordata_avro with key/value types of String / SensorData . The [SensorData] schema was created for demo purposes only and contains several fields to demonstrate KSML capabilities: { \"namespace\": \"io.axual.ksml.example\", \"doc\": \"Emulated sensor data with a few additional attributes\", \"name\": \"SensorData\", \"type\": \"record\", \"fields\": [ { \"doc\": \"The name of the sensor\", \"name\": \"name\", \"type\": \"string\" }, { \"doc\": \"The timestamp of the sensor reading\", \"name\": \"timestamp\", \"type\": \"long\" }, { \"doc\": \"The value of the sensor, represented as string\", \"name\": \"value\", \"type\": \"string\" }, { \"doc\": \"The type of the sensor\", \"name\": \"type\", \"type\": { \"name\": \"SensorType\", \"type\": \"enum\", \"doc\": \"The type of a sensor\", \"symbols\": [ \"AREA\", \"HUMIDITY\", \"LENGTH\", \"STATE\", \"TEMPERATURE\" ] } }, { \"doc\": \"The unit of the sensor\", \"name\": \"unit\", \"type\": \"string\" }, { \"doc\": \"The color of the sensor\", \"name\": \"color\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The city of the sensor\", \"name\": \"city\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The owner of the sensor\", \"name\": \"owner\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } For the rest of this document, we assume you have set up the ksml_sensordata_avro topic and populated it with some random data. So without any further delays, let's see how KSML allows us to process this data. KSML in practice Example 1. Inspect data on a topic The first example is one where we inspect data on a specific topic. The definition is as follows: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Multiple pipelines can be created in a single KSML definition consume_avro: from: sensor_source_avro forEach: code: log_message(key, value, format=\"AVRO\") Let's analyze this definition one element at a time. Before defining processing logic, we first define the streams used by the definition. In this case we define a stream named sensor_source_avro which reads from the topic ksml_sensordata_avro . The stream defines a string key and Avro SensorData values. Next is a list of functions that can be used by the processing logic. Here we define just one, log_message , which simply uses the provided logger to write the key, value and format of a message to the console. The third element pipelines defines the real processing logic. We define a pipeline called consume_avro , which takes messages from ksml_sensordata_avro and passes them to print_message . The definition file is parsed by KSML and translated into a Kafka Streams topology, which is described as follows: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro Processor: inspect_inspect_pipelines_consume_avro (stores: []) --> none <-- ksml_sensordata_avro And the output of the generated topology looks like this: 2024-03-06T18:31:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the output of the application is exactly that what we defined it to be in the log_message function, namely a dump of all data found on the topic. Example 2. Copying data to another topic Now that we can see what data is on a topic, we will start to manipulate its routing. In this example we are copying unmodified data to a secondary topic: streams: - topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData - topic: ksml_sensordata_copy keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_copy You can see that we specified a second stream named sensor_copy in this example, which is backed by the topic ksml_sensordata_copy target topic. The log_message function is unchanged, but the pipeline did undergo some changes. Two new elements are introduced here, namely via and to . The via tag allows users to define a series of operations executed on the data. In this case there is only one, namely a peek operation which does not modify any data, but simply outputs the data on stdout as a side effect. The to operation is a so-called \"sink operation\". Sink operations are always last in a pipeline. Processing of the pipeline does not continue after it was delivered to a sink operation. Note that in the first example above forEach is also a sink operation, whereas in this example we achieve the same result by passing the log_message function as a parameter to the peek operation. When this definition is translated by KSML, the following Kafka Streams topology is created: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- ksml_sensordata_avro Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_copy) <-- inspect_inspect_pipelines_consume_avro_via_1 The output is similar to that of example 1, but the same data can also be found on the ksml_sensordata_copy topic now. Example 3. Filtering data Now that we can read and write data, let's see if we can apply some logic to the processing as well. In this example we will be filtering data based on the contents of the value: # This example shows how to read from four simple streams and log all messages streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: filter if: sensor_is_blue - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_filtered Again, first we define the streams and the functions involved in the processing. You can see we added a new function called filter_message which returns true or false based on the color field in the value of the message. This function is used below in the pipeline. The pipeline is extended to include a filter operation, which takes a predicate function as parameter. That function is called for every input message. Only messages for which the function returns true are propagated. All other messages are discarded. Using this definition, KSML generates the following Kafka Streams topology: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_pipelines_consume_avro_via_2 <-- ksml_sensordata_avro Processor: inspect_inspect_pipelines_consume_avro_via_2 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- inspect_inspect_pipelines_consume_avro_via_1 Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- inspect_inspect_pipelines_consume_avro_via_2 When it executes, we see the following output: 2024-03-06T18:45:10,401Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:10,735Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,215Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,484Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,893Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:12,008Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the filter operation did its work. Only messages with field color set to blue are passed on to the peek operation, while other messages are discarded. Example 4. Branching messages Another way to filter messages is to use a branch operation. This is also a sink operation, which closes the processing of a pipeline. It is similar to forEach and to in that respect, but has a different definition and behaviour. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_blue: topic: ksml_sensordata_blue keyType: string valueType: avro:SensorData sensor_red: topic: ksml_sensordata_red keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) branch: - if: expression: value is not None and value[\"color\"] == \"blue\" to: sensor_blue - if: expression: value is not None and value[\"color\"] == \"red\" to: sensor_red - forEach: code: log.warn(\"UNKNOWN COLOR - {}\", value[\"color\"]) The branch operation takes a list of branches as its parameters, which each specifies a processing pipeline of its own. Branches contain the keyword if , which take a predicate function that determines if a message will flow into that particular branch, or if it will be passed to the next branch(es). Every message will only end up in one branch, namely the first one in order where the if predicate function returns true . In the example we see that the first branch will be populated only with messages with color field set to blue . Once there, these messages will be written to ksml_sensordata_blue . The second branch will only contain messages with color = red and these messages will be written to ksml_sensordata_red . Finally, the last branch outputs a message that the color is unknown and ends any further processing. When translated by KSML the following Kafka Streams topology is set up: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> branch_branch_pipelines_main_via_1 Processor: branch_branch_pipelines_main_via_1 (stores: []) --> branch_branch_branch_001 <-- ksml_sensordata_avro Processor: branch_branch_branch_001 (stores: []) --> branch_branch_branch_001-predicate-0, branch_branch_branch_001-predicate-1, branch_branch_branch_001-predicate-2 <-- branch_branch_pipelines_main_via_1 Processor: branch_branch_branch_001-predicate-0 (stores: []) --> branch_branch_ToOperationParser_001 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-1 (stores: []) --> branch_branch_ToOperationParser_002 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-2 (stores: []) --> branch_branch_pipelines_main_branch_3 <-- branch_branch_branch_001 Sink: branch_branch_ToOperationParser_001 (topic: ksml_sensordata_blue) <-- branch_branch_branch_001-predicate-0 Sink: branch_branch_ToOperationParser_002 (topic: ksml_sensordata_red) <-- branch_branch_branch_001-predicate-1 Processor: branch_branch_pipelines_main_branch_3 (stores: []) --> none <-- branch_branch_branch_001-predicate-2 It is clear that the branch operation is integrated in this topology. Its output looks like this: 2024-03-06T18:31:57,196Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,529Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:58,970Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,972Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:59,412Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} We see that every message processed by the pipeline is logged through the k.f.branch_pipelines_main_via_1_forEach logger. But the branch operation sorts the messages and sends messages with colors blue and red into their own branches. The only colors that show up as UNKNOWN COLOR - messages are non-blue and non-red and send through the branch_pipelines_main_branch_3_forEach logger. Example 5. Dynamic routing Sometimes it is necessary to route a message to one stream or another based on the content of a message. This example shows how to route messages dynamically using a TopicNameExtractor. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' The topicNameExtractor operation takes a function, which determines the routing of every message by returning a topic name string. In this case, when the key of a message is sensor1 then the message will be sent to ksml_sensordata_sensor1 . When it contains sensor2 the message is sent to ksml_sensordata_sensor2 . All other messages are sent to ksml_sensordata_sensor0 . The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> route_route_pipelines_main_via_1 Processor: route_route_pipelines_main_via_1 (stores: []) --> route_route_ToOperationParser_001 <-- ksml_sensordata_avro Sink: route_route_ToOperationParser_001 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@5d28e108) <-- route_route_pipelines_main_via_1 The output does not show anything special compared to previous examples, since all messages are simply written by the logger. Example 6. Multiple pipelines In the previous examples there was always a single pipeline definition for processing data. KSML allows us to define multiple pipelines in a single file. In this example we combine the filtering example with the routing example. We will also define new pipelines with the sole purpose of logging the routed messages. # This example shows how to route messages to a dynamic topic. The target topic is the result of an executed function. streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData sensor_0: topic: ksml_sensordata_sensor0 keyType: string valueType: avro:SensorData sensor_1: topic: ksml_sensordata_sensor1 keyType: string valueType: avro:SensorData sensor_2: topic: ksml_sensordata_sensor2 keyType: string valueType: avro:SensorData functions: # Only pass the message to the next step in the pipeline if the color is blue sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: filtering: from: sensor_source_avro via: - type: filter if: sensor_is_blue to: sensor_filtered routing: from: sensor_filtered via: - type: peek forEach: code: log.info(\"Routing Blue sensor - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' sensor0_peek: from: sensor_0 forEach: code: log.info(\"SENSOR0 - key={}, value={}\", key, value) sensor1_peek: from: sensor_1 forEach: code: log.info(\"SENSOR1 - key={}, value={}\", key, value) sensor2_peek: from: sensor_2 forEach: code: log.info(\"SENSOR2 - key={}, value={}\", key, value) In this definition we defined five pipelines: filtering which filters out all sensor messages that don't have the color blue and sends it to the sensor_filtered stream. routing which routes the data on the sensor_filtered stream to one of three target topics sensor0_peek which writes the content of the sensor_0 stream to the console sensor1_peek which writes the content of the sensor_1 stream to the console sensor2_peek which writes the content of the sensor_2 stream to the console The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> multiple_multiple_pipelines_filtering_via_1 Processor: multiple_multiple_pipelines_filtering_via_1 (stores: []) --> multiple_multiple_ToOperationParser_001 <-- ksml_sensordata_avro Sink: multiple_multiple_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- multiple_multiple_pipelines_filtering_via_1 Sub-topology: 1 Source: ksml_sensordata_filtered (topics: [ksml_sensordata_filtered]) --> multiple_multiple_pipelines_routing_via_1 Processor: multiple_multiple_pipelines_routing_via_1 (stores: []) --> multiple_multiple_ToOperationParser_002 <-- ksml_sensordata_filtered Sink: multiple_multiple_ToOperationParser_002 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@2700f556) <-- multiple_multiple_pipelines_routing_via_1 Sub-topology: 2 Source: ksml_sensordata_sensor0 (topics: [ksml_sensordata_sensor0]) --> multiple_multiple_pipelines_sensor0_peek Processor: multiple_multiple_pipelines_sensor0_peek (stores: []) --> none <-- ksml_sensordata_sensor0 Sub-topology: 3 Source: ksml_sensordata_sensor1 (topics: [ksml_sensordata_sensor1]) --> multiple_multiple_pipelines_sensor1_peek Processor: multiple_multiple_pipelines_sensor1_peek (stores: []) --> none <-- ksml_sensordata_sensor1 Sub-topology: 4 Source: ksml_sensordata_sensor2 (topics: [ksml_sensordata_sensor2]) --> multiple_multiple_pipelines_sensor2_peek Processor: multiple_multiple_pipelines_sensor2_peek (stores: []) --> none <-- ksml_sensordata_sensor2 And this is what the output would look something like this. The sensor peeks messages will not always be shown immediately after the Routing messages. This is because the pipelines are running in separate sub processes. 2024-03-06T20:11:39,520Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,523Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,533Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755889834, 'type': 'LENGTH', 'unit': 'm', 'value': '609', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,535Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755817913, 'type': 'STATE', 'unit': 'state', 'value': 'on', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,539Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,5419Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,546Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,549Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR2 - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,552Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,555Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,558Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,562Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"Introduction"},{"location":"introduction/#ksml-kafka-streams-for-low-code-environments","text":"","title":"KSML: Kafka Streams for Low Code Environments"},{"location":"introduction/#abstract","text":"Kafka Streams has captured the hearts and minds of many developers that want to develop streaming applications on top of Kafka. But as powerful as the framework is, Kafka Streams has had a hard time getting around the requirement of writing Java code and setting up build pipelines. There were some attempts to rebuild Kafka Streams, but up until now popular languages like Python did not receive equally powerful (and maintained) stream processing frameworks. In this article we will present a new declarative approach to unlock Kafka Streams, called KSML. By the time you finish reading this document, you will be able to write streaming applications yourself, using only a few simple basic rules and Python snippets. Setting up a test environment KSML in practice Example 1. Inspect data on a topic Example 2. Copying data to another topic Example 3. Filtering data Example 4. Branching messages Example 5. Dynamic routing Example 6. Multiple pipelines","title":"Abstract"},{"location":"introduction/#setting-up-a-test-environment","text":"To demonstrate KSML's capabilities, you will need a working Kafka cluster, or an Axual Platform/Cloud environment. Check out the Runners page to configure KSML. We set up a test topic, called ksml_sensordata_avro with key/value types of String / SensorData . The [SensorData] schema was created for demo purposes only and contains several fields to demonstrate KSML capabilities: { \"namespace\": \"io.axual.ksml.example\", \"doc\": \"Emulated sensor data with a few additional attributes\", \"name\": \"SensorData\", \"type\": \"record\", \"fields\": [ { \"doc\": \"The name of the sensor\", \"name\": \"name\", \"type\": \"string\" }, { \"doc\": \"The timestamp of the sensor reading\", \"name\": \"timestamp\", \"type\": \"long\" }, { \"doc\": \"The value of the sensor, represented as string\", \"name\": \"value\", \"type\": \"string\" }, { \"doc\": \"The type of the sensor\", \"name\": \"type\", \"type\": { \"name\": \"SensorType\", \"type\": \"enum\", \"doc\": \"The type of a sensor\", \"symbols\": [ \"AREA\", \"HUMIDITY\", \"LENGTH\", \"STATE\", \"TEMPERATURE\" ] } }, { \"doc\": \"The unit of the sensor\", \"name\": \"unit\", \"type\": \"string\" }, { \"doc\": \"The color of the sensor\", \"name\": \"color\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The city of the sensor\", \"name\": \"city\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"doc\": \"The owner of the sensor\", \"name\": \"owner\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } For the rest of this document, we assume you have set up the ksml_sensordata_avro topic and populated it with some random data. So without any further delays, let's see how KSML allows us to process this data.","title":"Setting up a test environment"},{"location":"introduction/#ksml-in-practice","text":"","title":"KSML in practice"},{"location":"introduction/#example-1-inspect-data-on-a-topic","text":"The first example is one where we inspect data on a specific topic. The definition is as follows: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Multiple pipelines can be created in a single KSML definition consume_avro: from: sensor_source_avro forEach: code: log_message(key, value, format=\"AVRO\") Let's analyze this definition one element at a time. Before defining processing logic, we first define the streams used by the definition. In this case we define a stream named sensor_source_avro which reads from the topic ksml_sensordata_avro . The stream defines a string key and Avro SensorData values. Next is a list of functions that can be used by the processing logic. Here we define just one, log_message , which simply uses the provided logger to write the key, value and format of a message to the console. The third element pipelines defines the real processing logic. We define a pipeline called consume_avro , which takes messages from ksml_sensordata_avro and passes them to print_message . The definition file is parsed by KSML and translated into a Kafka Streams topology, which is described as follows: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro Processor: inspect_inspect_pipelines_consume_avro (stores: []) --> none <-- ksml_sensordata_avro And the output of the generated topology looks like this: 2024-03-06T18:31:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the output of the application is exactly that what we defined it to be in the log_message function, namely a dump of all data found on the topic.","title":"Example 1. Inspect data on a topic"},{"location":"introduction/#example-2-copying-data-to-another-topic","text":"Now that we can see what data is on a topic, we will start to manipulate its routing. In this example we are copying unmodified data to a secondary topic: streams: - topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData - topic: ksml_sensordata_copy keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_copy You can see that we specified a second stream named sensor_copy in this example, which is backed by the topic ksml_sensordata_copy target topic. The log_message function is unchanged, but the pipeline did undergo some changes. Two new elements are introduced here, namely via and to . The via tag allows users to define a series of operations executed on the data. In this case there is only one, namely a peek operation which does not modify any data, but simply outputs the data on stdout as a side effect. The to operation is a so-called \"sink operation\". Sink operations are always last in a pipeline. Processing of the pipeline does not continue after it was delivered to a sink operation. Note that in the first example above forEach is also a sink operation, whereas in this example we achieve the same result by passing the log_message function as a parameter to the peek operation. When this definition is translated by KSML, the following Kafka Streams topology is created: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- ksml_sensordata_avro Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_copy) <-- inspect_inspect_pipelines_consume_avro_via_1 The output is similar to that of example 1, but the same data can also be found on the ksml_sensordata_copy topic now.","title":"Example 2. Copying data to another topic"},{"location":"introduction/#example-3-filtering-data","text":"Now that we can read and write data, let's see if we can apply some logic to the processing as well. In this example we will be filtering data based on the contents of the value: # This example shows how to read from four simple streams and log all messages streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData functions: # Log the message using the built-in log variable that is passed in from Java log_message: type: forEach parameters: - name: format type: string code: log.info(\"Consumed {} message - key={}, value={}\", format, key, value) sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: # Every pipeline logs its own message, passing in the format parameter to log_message above consume_avro: from: sensor_source_avro via: - type: filter if: sensor_is_blue - type: peek forEach: code: log_message(key, value, format=\"AVRO\") to: sensor_filtered Again, first we define the streams and the functions involved in the processing. You can see we added a new function called filter_message which returns true or false based on the color field in the value of the message. This function is used below in the pipeline. The pipeline is extended to include a filter operation, which takes a predicate function as parameter. That function is called for every input message. Only messages for which the function returns true are propagated. All other messages are discarded. Using this definition, KSML generates the following Kafka Streams topology: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> inspect_inspect_pipelines_consume_avro_via_1 Processor: inspect_inspect_pipelines_consume_avro_via_1 (stores: []) --> inspect_inspect_pipelines_consume_avro_via_2 <-- ksml_sensordata_avro Processor: inspect_inspect_pipelines_consume_avro_via_2 (stores: []) --> inspect_inspect_ToOperationParser_001 <-- inspect_inspect_pipelines_consume_avro_via_1 Sink: inspect_inspect_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- inspect_inspect_pipelines_consume_avro_via_2 When it executes, we see the following output: 2024-03-06T18:45:10,401Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:10,735Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,215Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,484Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:11,893Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:45:12,008Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} As you can see, the filter operation did its work. Only messages with field color set to blue are passed on to the peek operation, while other messages are discarded.","title":"Example 3. Filtering data"},{"location":"introduction/#example-4-branching-messages","text":"Another way to filter messages is to use a branch operation. This is also a sink operation, which closes the processing of a pipeline. It is similar to forEach and to in that respect, but has a different definition and behaviour. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_blue: topic: ksml_sensordata_blue keyType: string valueType: avro:SensorData sensor_red: topic: ksml_sensordata_red keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) branch: - if: expression: value is not None and value[\"color\"] == \"blue\" to: sensor_blue - if: expression: value is not None and value[\"color\"] == \"red\" to: sensor_red - forEach: code: log.warn(\"UNKNOWN COLOR - {}\", value[\"color\"]) The branch operation takes a list of branches as its parameters, which each specifies a processing pipeline of its own. Branches contain the keyword if , which take a predicate function that determines if a message will flow into that particular branch, or if it will be passed to the next branch(es). Every message will only end up in one branch, namely the first one in order where the if predicate function returns true . In the example we see that the first branch will be populated only with messages with color field set to blue . Once there, these messages will be written to ksml_sensordata_blue . The second branch will only contain messages with color = red and these messages will be written to ksml_sensordata_red . Finally, the last branch outputs a message that the color is unknown and ends any further processing. When translated by KSML the following Kafka Streams topology is set up: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> branch_branch_pipelines_main_via_1 Processor: branch_branch_pipelines_main_via_1 (stores: []) --> branch_branch_branch_001 <-- ksml_sensordata_avro Processor: branch_branch_branch_001 (stores: []) --> branch_branch_branch_001-predicate-0, branch_branch_branch_001-predicate-1, branch_branch_branch_001-predicate-2 <-- branch_branch_pipelines_main_via_1 Processor: branch_branch_branch_001-predicate-0 (stores: []) --> branch_branch_ToOperationParser_001 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-1 (stores: []) --> branch_branch_ToOperationParser_002 <-- branch_branch_branch_001 Processor: branch_branch_branch_001-predicate-2 (stores: []) --> branch_branch_pipelines_main_branch_3 <-- branch_branch_branch_001 Sink: branch_branch_ToOperationParser_001 (topic: ksml_sensordata_blue) <-- branch_branch_branch_001-predicate-0 Sink: branch_branch_ToOperationParser_002 (topic: ksml_sensordata_red) <-- branch_branch_branch_001-predicate-1 Processor: branch_branch_pipelines_main_branch_3 (stores: []) --> none <-- branch_branch_branch_001-predicate-2 It is clear that the branch operation is integrated in this topology. Its output looks like this: 2024-03-06T18:31:57,196Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:57,631Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,082Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,528Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,529Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:58,970Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T18:31:58,972Z WARN k.f.branch_pipelines_main_branch_3_forEach UNKNOWN COLOR - black 2024-03-06T18:31:59,412Z INFO k.f.branch_pipelines_main_via_1_forEach SOURCE MESSAGE - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} We see that every message processed by the pipeline is logged through the k.f.branch_pipelines_main_via_1_forEach logger. But the branch operation sorts the messages and sends messages with colors blue and red into their own branches. The only colors that show up as UNKNOWN COLOR - messages are non-blue and non-red and send through the branch_pipelines_main_branch_3_forEach logger.","title":"Example 4. Branching messages"},{"location":"introduction/#example-5-dynamic-routing","text":"Sometimes it is necessary to route a message to one stream or another based on the content of a message. This example shows how to route messages dynamically using a TopicNameExtractor. streams: sensor_source: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData pipelines: main: from: sensor_source via: - type: peek forEach: code: log.info(\"SOURCE MESSAGE - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' The topicNameExtractor operation takes a function, which determines the routing of every message by returning a topic name string. In this case, when the key of a message is sensor1 then the message will be sent to ksml_sensordata_sensor1 . When it contains sensor2 the message is sent to ksml_sensordata_sensor2 . All other messages are sent to ksml_sensordata_sensor0 . The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> route_route_pipelines_main_via_1 Processor: route_route_pipelines_main_via_1 (stores: []) --> route_route_ToOperationParser_001 <-- ksml_sensordata_avro Sink: route_route_ToOperationParser_001 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@5d28e108) <-- route_route_pipelines_main_via_1 The output does not show anything special compared to previous examples, since all messages are simply written by the logger.","title":"Example 5. Dynamic routing"},{"location":"introduction/#example-6-multiple-pipelines","text":"In the previous examples there was always a single pipeline definition for processing data. KSML allows us to define multiple pipelines in a single file. In this example we combine the filtering example with the routing example. We will also define new pipelines with the sole purpose of logging the routed messages. # This example shows how to route messages to a dynamic topic. The target topic is the result of an executed function. streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData sensor_filtered: topic: ksml_sensordata_filtered keyType: string valueType: avro:SensorData sensor_0: topic: ksml_sensordata_sensor0 keyType: string valueType: avro:SensorData sensor_1: topic: ksml_sensordata_sensor1 keyType: string valueType: avro:SensorData sensor_2: topic: ksml_sensordata_sensor2 keyType: string valueType: avro:SensorData functions: # Only pass the message to the next step in the pipeline if the color is blue sensor_is_blue: type: predicate code: | if value[\"color\"] == \"blue\": return True expression: False pipelines: filtering: from: sensor_source_avro via: - type: filter if: sensor_is_blue to: sensor_filtered routing: from: sensor_filtered via: - type: peek forEach: code: log.info(\"Routing Blue sensor - key={}, value={}\", key, value) to: topicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' if key == 'sensor2': return 'ksml_sensordata_sensor2' return 'ksml_sensordata_sensor0' sensor0_peek: from: sensor_0 forEach: code: log.info(\"SENSOR0 - key={}, value={}\", key, value) sensor1_peek: from: sensor_1 forEach: code: log.info(\"SENSOR1 - key={}, value={}\", key, value) sensor2_peek: from: sensor_2 forEach: code: log.info(\"SENSOR2 - key={}, value={}\", key, value) In this definition we defined five pipelines: filtering which filters out all sensor messages that don't have the color blue and sends it to the sensor_filtered stream. routing which routes the data on the sensor_filtered stream to one of three target topics sensor0_peek which writes the content of the sensor_0 stream to the console sensor1_peek which writes the content of the sensor_1 stream to the console sensor2_peek which writes the content of the sensor_2 stream to the console The equivalent Kafka Streams topology looks like this: Topologies: Sub-topology: 0 Source: ksml_sensordata_avro (topics: [ksml_sensordata_avro]) --> multiple_multiple_pipelines_filtering_via_1 Processor: multiple_multiple_pipelines_filtering_via_1 (stores: []) --> multiple_multiple_ToOperationParser_001 <-- ksml_sensordata_avro Sink: multiple_multiple_ToOperationParser_001 (topic: ksml_sensordata_filtered) <-- multiple_multiple_pipelines_filtering_via_1 Sub-topology: 1 Source: ksml_sensordata_filtered (topics: [ksml_sensordata_filtered]) --> multiple_multiple_pipelines_routing_via_1 Processor: multiple_multiple_pipelines_routing_via_1 (stores: []) --> multiple_multiple_ToOperationParser_002 <-- ksml_sensordata_filtered Sink: multiple_multiple_ToOperationParser_002 (extractor class: io.axual.ksml.user.UserTopicNameExtractor@2700f556) <-- multiple_multiple_pipelines_routing_via_1 Sub-topology: 2 Source: ksml_sensordata_sensor0 (topics: [ksml_sensordata_sensor0]) --> multiple_multiple_pipelines_sensor0_peek Processor: multiple_multiple_pipelines_sensor0_peek (stores: []) --> none <-- ksml_sensordata_sensor0 Sub-topology: 3 Source: ksml_sensordata_sensor1 (topics: [ksml_sensordata_sensor1]) --> multiple_multiple_pipelines_sensor1_peek Processor: multiple_multiple_pipelines_sensor1_peek (stores: []) --> none <-- ksml_sensordata_sensor1 Sub-topology: 4 Source: ksml_sensordata_sensor2 (topics: [ksml_sensordata_sensor2]) --> multiple_multiple_pipelines_sensor2_peek Processor: multiple_multiple_pipelines_sensor2_peek (stores: []) --> none <-- ksml_sensordata_sensor2 And this is what the output would look something like this. The sensor peeks messages will not always be shown immediately after the Routing messages. This is because the pipelines are running in separate sub processes. 2024-03-06T20:11:39,520Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,523Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor6, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor6', 'owner': 'Charlie', 'timestamp': 1709755877401, 'type': 'LENGTH', 'unit': 'ft', 'value': '507', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,533Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755889834, 'type': 'LENGTH', 'unit': 'm', 'value': '609', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,535Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Evan', 'timestamp': 1709755817913, 'type': 'STATE', 'unit': 'state', 'value': 'on', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,539Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,5419Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor7, value={'city': 'Utrecht', 'color': 'blue', 'name': 'sensor7', 'owner': 'Evan', 'timestamp': 1709755892051, 'type': 'HUMIDITY', 'unit': '%', 'value': '77', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,546Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,549Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR2 - key=sensor2, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor2', 'owner': 'Bob', 'timestamp': 1709755893390, 'type': 'HUMIDITY', 'unit': '%', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,552Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,555Z INFO k.f.route2_pipelines_sensor1_peek_forEach SENSOR1 - key=sensor1, value={'city': 'Xanten', 'color': 'blue', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709755894717, 'type': 'HUMIDITY', 'unit': '%', 'value': '76', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,558Z INFO k.f.route2_pipelines_routing_via_1_forEach Routing Blue sensor - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:11:39,562Z INFO k.f.route2_pipelines_sensor0_peek_forEach SENSOR0 - key=sensor9, value={'city': 'Alkmaar', 'color': 'blue', 'name': 'sensor9', 'owner': 'Alice', 'timestamp': 1709755896937, 'type': 'HUMIDITY', 'unit': '%', 'value': '65', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"Example 6. Multiple pipelines"},{"location":"ksml-language-spec/","text":"TopologyDefinition KSML definition Properties functions (object) : (optional) Functions that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/AggregatorDefinition . object : Refer to #/definitions/ForEachActionDefinition . object : Refer to #/definitions/ForeignKeyExtractorDefinition . object : Refer to #/definitions/GeneratorDefinition . object : Refer to #/definitions/GenericFunctionDefinitionWithImplicitStoreType . object : Refer to #/definitions/InitializerDefinition . object : Refer to #/definitions/KeyTransformerDefinition . object : Refer to #/definitions/KeyValueMapperDefinition . object : Refer to #/definitions/KeyValuePrinterDefinition . object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinition . object : Refer to #/definitions/KeyValueToValueListTransformerDefinition . object : Refer to #/definitions/KeyValueTransformerDefinition . object : Refer to #/definitions/MergerDefinition . object : Refer to #/definitions/MetadataTransformerDefinition . object : Refer to #/definitions/PredicateDefinition . object : Refer to #/definitions/ReducerDefinition . object : Refer to #/definitions/StreamPartitionerDefinition . object : Refer to #/definitions/TimestampExtractorDefinition . object : Refer to #/definitions/TopicNameExtractorDefinition . object : Refer to #/definitions/ValueJoinerDefinition . object : Refer to #/definitions/ValueTransformerDefinition . globalTables (object) : (optional) GlobalTables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/GlobalTableDefinition . pipelines (object) : (optional) Collection of named pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/PipelineDefinition . producers (object) : (optional) Collection of named producers. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/ProducerDefinition . stores (object) : (optional) State stores that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . streams (object) : (optional) Streams that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/StreamDefinition . tables (object) : (optional) Tables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/TableDefinition . Definitions AggregateOperation (object) : An aggregate operation. Cannot contain additional properties. adder : (optional) (GroupedTable) A function that adds a record to the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . aggregator : (optional) (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . initializer : The initializer function, which generates an initial value for every set of aggregated records. Any of string object : Refer to #/definitions/InitializerDefinitionWithImplicitStoreType . merger : (optional) (SessionWindowedStream, SessionWindowedCogroupedStream) A function that combines two aggregation results. Any of string object : Refer to #/definitions/MergerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . subtractor : (optional) (GroupedTable) A function that removes a record from the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"aggregate\"] . AggregatorDefinition (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The expression returned by the aggregator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"aggregator\"] . AggregatorDefinitionWithImplicitStoreType (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The expression returned by the aggregator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. BranchDefinitionWithPipeline (object) : Defines a branch with sub-pipeline in a BranchOperation. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/StringOrInlinePredicateDefinitionWithImplicitStoreType . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . CogroupOperation (object) : A cogroup operation. Cannot contain additional properties. aggregator : (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"cogroup\"] . ConvertKeyOperation (object) : An operation to convert the stream key type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream key into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKey\"] . ConvertKeyValueOperation (object) : An operation to convert the stream key and value types to other types. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The tuple type to convert the stream key/value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKeyValue\"] . ConvertValueOperation (object) : An operation to convert the stream value type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertValue\"] . CountOperation (object) : Count the number of times a key is seen in a given window. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the count operation's result. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"count\"] . FilterNotOperation (object) : Filter records based on the inverse result of a predicate function. Cannot contain additional properties. if : A function that returns \"false\" when records are accepted, \"true\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filterNot\"] . FilterOperation (object) : Filter records based on a predicate function. Cannot contain additional properties. if : A function that returns \"true\" when records are accepted, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filter\"] . ForEachActionDefinition (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The expression returned by the foreach action. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"forEach\"] . ForEachActionDefinitionWithImplicitStoreType (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The expression returned by the foreach action. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) ForeignKeyExtractorDefinition (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The expression returned by the foreign key extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"foreignKeyExtractor\"] . ForeignKeyExtractorDefinitionWithImplicitStoreType (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The expression returned by the foreign key extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. GeneratorDefinition (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The expression returned by the message generator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generator\"] . GeneratorDefinitionWithImplicitStoreType (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The expression returned by the message generator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. GenericFunctionDefinitionWithImplicitStoreType (object) : Defines a generic function function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the generic function. Any of boolean integer number string expression : (optional) The expression returned by the generic function. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the generic function. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the generic function. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the generic function. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the generic function. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generic\"] . GlobalTableDefinition (object) : Contains a definition of a GlobalTable, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the global table. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this global table. valueType (string, required) : The value type of the global table. GlobalTableDefinitionAsJoinTarget (object) : Reference to a GlobalTable in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the global table. store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this global table. valueType (string) : (optional) The value type of the global table. GroupByKeyOperation (object) : Operation to group all messages with the same key together. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupByKey\"] . GroupByOperation (object) : Operation to group all messages with together based on a keying function. Cannot contain additional properties. mapper : Function to map records to a key they can be grouped on. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream or table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupBy\"] . InitializerDefinition (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The expression returned by the initializer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"initializer\"] . InitializerDefinitionWithImplicitStoreType (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The expression returned by the initializer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. JoinWithGlobalTableOperation (object) : Operation to join with a table. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream to the primary key type of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithStreamOperation (object) : Operation to join with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the joined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to join with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a join over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithTableOperation (object) : Operation to join with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the joined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . KeyTransformerDefinition (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The expression returned by the key transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyTransformer\"] . KeyTransformerDefinitionWithImplicitStoreType (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The expression returned by the key transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueMapperDefinition (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue mapper. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValueMapper\"] . KeyValueMapperDefinitionWithImplicitStoreType (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue mapper. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. KeyValuePrinterDefinition (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue printer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValuePrinter\"] . KeyValuePrinterDefinitionWithImplicitStoreType (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue printer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. KeyValueStateStoreDefinition (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueToKeyValueListTransformerDefinition (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-keyvaluelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToKeyValueListTransformer\"] . KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-keyvaluelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueToValueListTransformerDefinition (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-valuelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToValueListTransformer\"] . KeyValueToValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-valuelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueTransformerDefinition (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueTransformer\"] . KeyValueTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) LeftJoinWithGlobalTableOperation (object) : Operation to leftJoin with a globalTable. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream with the primary key of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithStreamOperation (object) : Operation to leftJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the leftJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to leftJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a leftJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithTableOperation (object) : Operation to leftJoin with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the leftJoined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . MergeOperation (object) : A merge operation to join two Streams. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. stream : The stream to merge with. Any of string object : Refer to #/definitions/StreamDefinition . type : The type of the operation. Must be one of: [\"merge\"] . MergerDefinition (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The expression returned by the merger. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"merger\"] . MergerDefinitionWithImplicitStoreType (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The expression returned by the merger. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. MetadataTransformerDefinition (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The expression returned by the metadata transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"metadataTransformer\"] . MetadataTransformerDefinitionWithImplicitStoreType (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The expression returned by the metadata transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) OuterJoinWithStreamOperation (object) : Operation to outerJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the outerJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to outerJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for an outerJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . OuterJoinWithTableOperation (object) : Operation to outerJoin with a table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the outerJoined table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to outerJoin with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . ParameterDefinition (object) : Defines a parameter for a user function. Cannot contain additional properties. defaultValue (string) : (optional) The default value for the parameter. name (string, required) : The name of the parameter. type (string, required) : The type of the parameter. PeekOperation (object) : Operation to peek into a stream, without modifying the stream contents. Cannot contain additional properties. forEach : A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"peek\"] . PipelineDefinition (object) : Defines a pipeline through a source, a series of operations to perform on it and a sink operation to close the stream with. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/BranchDefinitionWithPipeline . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . from : Pipeline source. Any of string object : Refer to #/definitions/TopicDefinitionSource . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . PredicateDefinition (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The expression returned by the predicate. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"predicate\"] . PredicateDefinitionWithImplicitStoreType (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The expression returned by the predicate. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) PrintOperation (object) : Operation to print the contents of a pipeline on the screen or to write them to a file. Cannot contain additional properties. filename (string) : (optional) The filename to output records to. If nothing is specified, then messages will be printed on stdout. label (string) : (optional) A label to attach to the output records. mapper : (optional) A function to convert record into a string for output. Any of string object : Refer to #/definitions/KeyValuePrinterDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. ProducerDefinition (object) : Definition of a Producer that regularly generates messages for a topic. Cannot contain additional properties. batchSize (integer) : (optional) The size of batches. condition : (optional) A function that validates the generator's result message. Returns \"true\" when the message may be produced on the topic, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . count (integer) : (optional) The number of messages to produce. generator : The function that generates records. Any of string object : Refer to #/definitions/GeneratorDefinitionWithImplicitStoreType . interval : (optional) The interval with which the generator is called. Any of integer string to : The topic to produce to. Any of string object : Refer to #/definitions/TopicDefinition . until : (optional) A predicate that returns true to indicate producing should stop. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . ReduceOperationWithAdderAndSubtractor (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. adder : A function that adds a record to the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . subtractor : A function that removes a record from the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"reduce\"] . ReduceOperationWithReducer (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. reducer : A function that computes a new aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"reduce\"] . ReducerDefinition (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The expression returned by the reducer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"reducer\"] . ReducerDefinitionWithImplicitStoreType (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The expression returned by the reducer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. RepartitionOperation (object) : Operation to (re)partition a stream. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. numberOfPartitions (integer) : (optional) The target number of partitions. partitioner : (optional) A function that partitions stream records. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"repartition\"] . SessionStateStoreDefinition (object) : Definition of a session state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the session store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the session store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this session store, \"false\" otherwise. name (string) : (optional) The name of the session store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this session store needs to be stored on disk, \"false\" otherwise. retention : (optional) The duration for which elements in the session store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"session\"] . valueType (string) : (optional) The value type of the session store. StreamDefinition (object) : Contains a definition of a Stream, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the stream. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this stream. valueType (string, required) : The value type of the stream. StreamDefinitionAsJoinTarget (object) : Reference to a Stream in a join or merge operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the stream. topic (string, required) : The name of the Kafka topic for this stream. valueType (string) : (optional) The value type of the stream. StreamPartitionerDefinition (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The expression returned by the stream partitioner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"streamPartitioner\"] . StreamPartitionerDefinitionWithImplicitStoreType (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The expression returned by the stream partitioner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. StringOrInlinePredicateDefinitionWithImplicitStoreType (object) : Defines the condition under which messages get sent down this branch. Cannot contain additional properties. if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . SuppressOperationUntilTimeLimit (object) : Operation to suppress messages in the source stream until a time limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . duration : The duration for which messages are suppressed. Any of integer string maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"timeLimit\"] . SuppressOperationUntilWindowCloses (object) : Operation to suppress messages in the source stream until a window limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"windowCloses\"] . TableDefinition (object) : Contains a definition of a Table, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the table. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this table. valueType (string, required) : The value type of the table. TableDefinitionAsJoinTarget (object) : Reference to a Table in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the table. store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this table. valueType (string) : (optional) The value type of the table. TimestampExtractorDefinition (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The expression returned by the timestamp extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"timestampExtractor\"] . TimestampExtractorDefinitionWithImplicitStoreType (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The expression returned by the timestamp extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. ToStreamOperation (object) : Convert a Table into a Stream, optionally through a custom key transformer. Cannot contain additional properties. mapper : (optional) A function that computes the output key for every record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"toStream\"] . ToTableOperation (object) : Convert a Stream into a Table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"toTable\"] . ToTopicDefinition (object) : Writes out pipeline messages to a topic. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. ToTopicNameExtractorDefinition (object) : Writes out pipeline messages to a topic as given by a topic name extractor. Cannot contain additional properties. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topicNameExtractor : Reference to a pre-defined topic name extractor, or an inline definition of a topic name extractor. Any of string object : Refer to #/definitions/TopicNameExtractorDefinitionWithImplicitStoreType . TopicDefinition (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. TopicDefinitionSource (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the topic. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string, required) : The value type of the topic. TopicNameExtractorDefinition (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The expression returned by the topic name extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"topicNameExtractor\"] . TopicNameExtractorDefinitionWithImplicitStoreType (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The expression returned by the topic name extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. TransformKeyOperation (object) : Convert the key of every record in the stream to another key. Cannot contain additional properties. mapper : A function that computes a new key for each record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKey\", \"mapKey\", \"selectKey\"] . TransformKeyValueOperation (object) : Convert the key/value of every record in the stream to another key/value. Cannot contain additional properties. mapper : A function that computes a new key/value for each record. Any of string object : Refer to #/definitions/KeyValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"map\", \"transformKeyValue\"] . TransformKeyValueToKeyValueListOperation (object) : Convert a stream by transforming every record into a list of derived records. Cannot contain additional properties. mapper : A function that converts every record of a stream to a list of output records. Any of string object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToKeyValueList\", \"flatMap\"] . TransformKeyValueToValueListOperation (object) : Convert every record in the stream to a list of output records with the same key. Cannot contain additional properties. mapper : A function that converts every key/value into a list of result values, each of which will be combined with the original key to form a new message in the output stream. Any of string object : Refer to #/definitions/KeyValueToValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToValueList\", \"flatMapValues\"] . TransformMetadataOperation (object) : Convert the metadata of every record in the stream. Cannot contain additional properties. mapper : A function that converts the metadata (Kafka headers, timestamp) of every record in the stream. Any of string object : Refer to #/definitions/MetadataTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformMetadata\"] . TransformValueOperation (object) : Convert the value of every record in the stream to another value. Cannot contain additional properties. mapper : A function that converts the value of every record into another value. Any of string object : Refer to #/definitions/ValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the transformed table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"mapValue\", \"transformValue\", \"mapValues\"] . ValueJoinerDefinition (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The expression returned by the value joiner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"valueJoiner\"] . ValueJoinerDefinitionWithImplicitStoreType (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The expression returned by the value joiner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. ValueTransformerDefinition (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The expression returned by the value transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"valueTransformer\"] . ValueTransformerDefinitionWithImplicitStoreType (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The expression returned by the value transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) WindowBySessionOperation (object) : Operation to window messages by session, configured by an inactivity gap. Cannot contain additional properties. grace : (optional) (Tumbling, Hopping) The grace period, during which out-of-order records can still be processed. Any of integer string inactivityGap : The inactivity gap, below which two messages are considered to be of the same session. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowBySession\"] . WindowByTimeOperationWithHoppingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. advanceBy : The amount of time to increase time windows by. Any of integer string duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"hopping\"] . WindowByTimeOperationWithSlidingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. timeDifference : The maximum amount of time difference between two records. Any of integer string type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"sliding\"] . WindowByTimeOperationWithTumblingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"tumbling\"] . WindowStateStoreDefinition (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string WindowStateStoreDefinitionWithImplicitStoreType (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string","title":"Language specification"},{"location":"ksml-language-spec/#topologydefinition","text":"KSML definition","title":"TopologyDefinition"},{"location":"ksml-language-spec/#properties","text":"functions (object) : (optional) Functions that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/AggregatorDefinition . object : Refer to #/definitions/ForEachActionDefinition . object : Refer to #/definitions/ForeignKeyExtractorDefinition . object : Refer to #/definitions/GeneratorDefinition . object : Refer to #/definitions/GenericFunctionDefinitionWithImplicitStoreType . object : Refer to #/definitions/InitializerDefinition . object : Refer to #/definitions/KeyTransformerDefinition . object : Refer to #/definitions/KeyValueMapperDefinition . object : Refer to #/definitions/KeyValuePrinterDefinition . object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinition . object : Refer to #/definitions/KeyValueToValueListTransformerDefinition . object : Refer to #/definitions/KeyValueTransformerDefinition . object : Refer to #/definitions/MergerDefinition . object : Refer to #/definitions/MetadataTransformerDefinition . object : Refer to #/definitions/PredicateDefinition . object : Refer to #/definitions/ReducerDefinition . object : Refer to #/definitions/StreamPartitionerDefinition . object : Refer to #/definitions/TimestampExtractorDefinition . object : Refer to #/definitions/TopicNameExtractorDefinition . object : Refer to #/definitions/ValueJoinerDefinition . object : Refer to #/definitions/ValueTransformerDefinition . globalTables (object) : (optional) GlobalTables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/GlobalTableDefinition . pipelines (object) : (optional) Collection of named pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/PipelineDefinition . producers (object) : (optional) Collection of named producers. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/ProducerDefinition . stores (object) : (optional) State stores that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ Any of object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . streams (object) : (optional) Streams that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/StreamDefinition . tables (object) : (optional) Tables that can be referenced in producers and pipelines. ^[a-zA-Z0-9_]+$ (object) : Refer to #/definitions/TableDefinition .","title":"Properties"},{"location":"ksml-language-spec/#definitions","text":"AggregateOperation (object) : An aggregate operation. Cannot contain additional properties. adder : (optional) (GroupedTable) A function that adds a record to the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . aggregator : (optional) (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . initializer : The initializer function, which generates an initial value for every set of aggregated records. Any of string object : Refer to #/definitions/InitializerDefinitionWithImplicitStoreType . merger : (optional) (SessionWindowedStream, SessionWindowedCogroupedStream) A function that combines two aggregation results. Any of string object : Refer to #/definitions/MergerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . subtractor : (optional) (GroupedTable) A function that removes a record from the aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"aggregate\"] . AggregatorDefinition (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The expression returned by the aggregator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"aggregator\"] . AggregatorDefinitionWithImplicitStoreType (object) : Defines a aggregator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the aggregator. Any of boolean integer number string expression : (optional) The expression returned by the aggregator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the aggregator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the aggregator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the aggregator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the aggregator. Only required for function types, which are not pre-defined. BranchDefinitionWithPipeline (object) : Defines a branch with sub-pipeline in a BranchOperation. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/StringOrInlinePredicateDefinitionWithImplicitStoreType . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . CogroupOperation (object) : A cogroup operation. Cannot contain additional properties. aggregator : (GroupedStream, SessionWindowedStream, TimeWindowedStream) The aggregator function, which combines a value with the previous aggregation result and outputs a new aggregation result. Any of string object : Refer to #/definitions/AggregatorDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"cogroup\"] . ConvertKeyOperation (object) : An operation to convert the stream key type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream key into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKey\"] . ConvertKeyValueOperation (object) : An operation to convert the stream key and value types to other types. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The tuple type to convert the stream key/value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertKeyValue\"] . ConvertValueOperation (object) : An operation to convert the stream value type to another type. Conversion is only syntactic, eg. from Avro to XML. Cannot contain additional properties. into (string, required) : The type to convert the stream value into. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"convertValue\"] . CountOperation (object) : Count the number of times a key is seen in a given window. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the count operation's result. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"count\"] . FilterNotOperation (object) : Filter records based on the inverse result of a predicate function. Cannot contain additional properties. if : A function that returns \"false\" when records are accepted, \"true\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filterNot\"] . FilterOperation (object) : Filter records based on a predicate function. Cannot contain additional properties. if : A function that returns \"true\" when records are accepted, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the filtered table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"filter\"] . ForEachActionDefinition (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The expression returned by the foreach action. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"forEach\"] . ForEachActionDefinitionWithImplicitStoreType (object) : Defines a foreach action function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreach action. Any of boolean integer number string expression : (optional) The expression returned by the foreach action. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreach action. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreach action. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreach action. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreach action. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the foreach action uses. Only required if the function wants to use a state store. Items (string) ForeignKeyExtractorDefinition (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The expression returned by the foreign key extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"foreignKeyExtractor\"] . ForeignKeyExtractorDefinitionWithImplicitStoreType (object) : Defines a foreign key extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the foreign key extractor. Any of boolean integer number string expression : (optional) The expression returned by the foreign key extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the foreign key extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the foreign key extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the foreign key extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the foreign key extractor. Only required for function types, which are not pre-defined. GeneratorDefinition (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The expression returned by the message generator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generator\"] . GeneratorDefinitionWithImplicitStoreType (object) : Defines a message generator function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the message generator. Any of boolean integer number string expression : (optional) The expression returned by the message generator. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the message generator. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the message generator. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the message generator. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the message generator. Only required for function types, which are not pre-defined. GenericFunctionDefinitionWithImplicitStoreType (object) : Defines a generic function function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the generic function. Any of boolean integer number string expression : (optional) The expression returned by the generic function. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the generic function. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the generic function. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the generic function. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the generic function. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"generic\"] . GlobalTableDefinition (object) : Contains a definition of a GlobalTable, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the global table. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this global table. valueType (string, required) : The value type of the global table. GlobalTableDefinitionAsJoinTarget (object) : Reference to a GlobalTable in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the global table. store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this global table. valueType (string) : (optional) The value type of the global table. GroupByKeyOperation (object) : Operation to group all messages with the same key together. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupByKey\"] . GroupByOperation (object) : Operation to group all messages with together based on a keying function. Cannot contain additional properties. mapper : Function to map records to a key they can be grouped on. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the grouped stream or table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"groupBy\"] . InitializerDefinition (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The expression returned by the initializer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"initializer\"] . InitializerDefinitionWithImplicitStoreType (object) : Defines a initializer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the initializer. Any of boolean integer number string expression : (optional) The expression returned by the initializer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the initializer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the initializer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the initializer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the initializer. Only required for function types, which are not pre-defined. JoinWithGlobalTableOperation (object) : Operation to join with a table. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream to the primary key type of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithStreamOperation (object) : Operation to join with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the joined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to join with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a join over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . JoinWithTableOperation (object) : Operation to join with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the joined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"join\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . KeyTransformerDefinition (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The expression returned by the key transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyTransformer\"] . KeyTransformerDefinitionWithImplicitStoreType (object) : Defines a key transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the key transformer. Any of boolean integer number string expression : (optional) The expression returned by the key transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the key transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the key transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the key transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the key transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the key transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueMapperDefinition (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue mapper. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValueMapper\"] . KeyValueMapperDefinitionWithImplicitStoreType (object) : Defines a keyvalue mapper function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue mapper. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue mapper. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue mapper. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue mapper. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue mapper. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue mapper. Only required for function types, which are not pre-defined. KeyValuePrinterDefinition (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue printer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"keyValuePrinter\"] . KeyValuePrinterDefinitionWithImplicitStoreType (object) : Defines a keyvalue printer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue printer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue printer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue printer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue printer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue printer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue printer. Only required for function types, which are not pre-defined. KeyValueStateStoreDefinition (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string keyType (string) : (optional) The key type of the keyValue store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. name (string) : (optional) The name of the keyValue store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . valueType (string) : (optional) The value type of the keyValue store. versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType (object) : Definition of a keyValue state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the keyValue store need to be buffered and periodically released, \"false\" to emit all changes directly. historyRetention : (optional) (Versioned only) The duration for which old record versions are available for query (cannot be negative). Any of integer string logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this keyValue store, \"false\" otherwise. persistent (boolean) : (optional) \"true\" if this keyValue store needs to be stored on disk, \"false\" otherwise. segmentInterval : (optional) Size of segments for storing old record versions (must be positive). Old record versions for the same key in a single segment are stored (updated and accessed) together. The only impact of this parameter is performance. If segments are large and a workload results in many record versions for the same key being collected in a single segment, performance may degrade as a result. On the other hand, historical reads (which access older segments) and out-of-order writes may slow down if there are too many segments. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"keyValue\"] . versioned (boolean) : (optional) \"true\" if elements in the store are versioned, \"false\" otherwise. KeyValueToKeyValueListTransformerDefinition (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-keyvaluelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToKeyValueListTransformer\"] . KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-keyvaluelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-keyvaluelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-keyvaluelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-keyvaluelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-keyvaluelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-keyvaluelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-keyvaluelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-keyvaluelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueToValueListTransformerDefinition (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-valuelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueToValueListTransformer\"] . KeyValueToValueListTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue-to-valuelist transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue-to-valuelist transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue-to-valuelist transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue-to-valuelist transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue-to-valuelist transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue-to-valuelist transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue-to-valuelist transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue-to-valuelist transformer uses. Only required if the function wants to use a state store. Items (string) KeyValueTransformerDefinition (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"keyValueTransformer\"] . KeyValueTransformerDefinitionWithImplicitStoreType (object) : Defines a keyvalue transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the keyvalue transformer. Any of boolean integer number string expression : (optional) The expression returned by the keyvalue transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the keyvalue transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the keyvalue transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the keyvalue transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the keyvalue transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the keyvalue transformer uses. Only required if the function wants to use a state store. Items (string) LeftJoinWithGlobalTableOperation (object) : Operation to leftJoin with a globalTable. Cannot contain additional properties. globalTable : A reference to the globalTable, or an inline definition of the globalTable to join with. Any of string object : Refer to #/definitions/GlobalTableDefinitionAsJoinTarget . mapper : A function that maps the key value from the stream with the primary key of the globalTable. Any of string object : Refer to #/definitions/KeyValueMapperDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithStreamOperation (object) : Operation to leftJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the leftJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to leftJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for a leftJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . LeftJoinWithTableOperation (object) : Operation to leftJoin with a table. Cannot contain additional properties. foreignKeyExtractor : (optional) A function that can translate the join table value to a primary key. Any of string object : Refer to #/definitions/ForeignKeyExtractorDefinitionWithImplicitStoreType . grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherPartitioner : (optional) A function that partitions the records on the join table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . partitioner : (optional) A function that partitions the records on the primary table. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the leftJoined table (only used for Table-Table joins). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to join with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"leftJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . MergeOperation (object) : A merge operation to join two Streams. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. stream : The stream to merge with. Any of string object : Refer to #/definitions/StreamDefinition . type : The type of the operation. Must be one of: [\"merge\"] . MergerDefinition (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The expression returned by the merger. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"merger\"] . MergerDefinitionWithImplicitStoreType (object) : Defines a merger function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the merger. Any of boolean integer number string expression : (optional) The expression returned by the merger. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the merger. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the merger. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the merger. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the merger. Only required for function types, which are not pre-defined. MetadataTransformerDefinition (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The expression returned by the metadata transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"metadataTransformer\"] . MetadataTransformerDefinitionWithImplicitStoreType (object) : Defines a metadata transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the metadata transformer. Any of boolean integer number string expression : (optional) The expression returned by the metadata transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the metadata transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the metadata transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the metadata transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the metadata transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the metadata transformer uses. Only required if the function wants to use a state store. Items (string) OuterJoinWithStreamOperation (object) : Operation to outerJoin with a stream. Cannot contain additional properties. grace : (optional) The window grace period (the time to admit out-of-order events after the end of the window). Any of integer string name (string) : (optional) The name of the operation processor. otherStore : Materialized view of the outerJoined stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . stream : A reference to the stream, or an inline definition of the stream to outerJoin with. Any of string object : Refer to #/definitions/StreamDefinitionAsJoinTarget . thisStore : Materialized view of the source stream. Any of string object : Refer to #/definitions/WindowStateStoreDefinitionWithImplicitStoreType . timeDifference : The maximum time difference for an outerJoin over two streams on the same key. Any of integer string type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . OuterJoinWithTableOperation (object) : Operation to outerJoin with a table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the outerJoined table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . table : A reference to the table, or an inline definition of the table to outerJoin with. Any of string object : Refer to #/definitions/TableDefinitionAsJoinTarget . type : The type of the operation. Must be one of: [\"outerJoin\"] . valueJoiner : A function that joins two values. Any of string object : Refer to #/definitions/ValueJoinerDefinitionWithImplicitStoreType . ParameterDefinition (object) : Defines a parameter for a user function. Cannot contain additional properties. defaultValue (string) : (optional) The default value for the parameter. name (string, required) : The name of the parameter. type (string, required) : The type of the parameter. PeekOperation (object) : Operation to peek into a stream, without modifying the stream contents. Cannot contain additional properties. forEach : A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"peek\"] . PipelineDefinition (object) : Defines a pipeline through a source, a series of operations to perform on it and a sink operation to close the stream with. Cannot contain additional properties. as (string) : (optional) The name to register the pipeline result under, which can be used as source by follow-up pipelines. branch (array) : (optional) Defines a single branch, consisting of a condition and a pipeline to execute for messages that fulfil the predicate. Items (object) : Refer to #/definitions/BranchDefinitionWithPipeline . forEach : (optional) A function that gets called for every message in the stream. Any of string object : Refer to #/definitions/ForEachActionDefinitionWithImplicitStoreType . from : Pipeline source. Any of string object : Refer to #/definitions/TopicDefinitionSource . name (string) : (optional) The name of the operation processor. print (object) : (optional) The specification of where to print messages to. Refer to #/definitions/PrintOperation . to : (optional) Ends the pipeline by sending all messages to a stream, table or globalTable, or to an inline defined output topic and optional partitioner. Any of string object : Refer to #/definitions/ToTopicDefinition . toTopicNameExtractor : (optional) Ends the pipeline by sending all messages to a topic provided by a pre-defined topic name extractor function, or to a topic provided by an inline defined topic name extractor and optional partitioner. Any of string object : Refer to #/definitions/ToTopicNameExtractorDefinition . via (array) : (optional) A series of operations performed on the input stream. Items Any of object : Refer to #/definitions/AggregateOperation . object : Refer to #/definitions/CogroupOperation . object : Refer to #/definitions/ConvertKeyOperation . object : Refer to #/definitions/ConvertKeyValueOperation . object : Refer to #/definitions/ConvertValueOperation . object : Refer to #/definitions/CountOperation . object : Refer to #/definitions/FilterNotOperation . object : Refer to #/definitions/FilterOperation . object : Refer to #/definitions/GroupByKeyOperation . object : Refer to #/definitions/GroupByOperation . object : Refer to #/definitions/JoinWithGlobalTableOperation . object : Refer to #/definitions/JoinWithStreamOperation . object : Refer to #/definitions/JoinWithTableOperation . object : Refer to #/definitions/LeftJoinWithGlobalTableOperation . object : Refer to #/definitions/LeftJoinWithStreamOperation . object : Refer to #/definitions/LeftJoinWithTableOperation . object : Refer to #/definitions/MergeOperation . object : Refer to #/definitions/OuterJoinWithStreamOperation . object : Refer to #/definitions/OuterJoinWithTableOperation . object : Refer to #/definitions/PeekOperation . object : Refer to #/definitions/ReduceOperationWithAdderAndSubtractor . object : Refer to #/definitions/ReduceOperationWithReducer . object : Refer to #/definitions/RepartitionOperation . object : Refer to #/definitions/SuppressOperationUntilTimeLimit . object : Refer to #/definitions/SuppressOperationUntilWindowCloses . object : Refer to #/definitions/ToStreamOperation . object : Refer to #/definitions/ToTableOperation . object : Refer to #/definitions/TransformKeyOperation . object : Refer to #/definitions/TransformKeyValueOperation . object : Refer to #/definitions/TransformKeyValueToKeyValueListOperation . object : Refer to #/definitions/TransformKeyValueToValueListOperation . object : Refer to #/definitions/TransformMetadataOperation . object : Refer to #/definitions/TransformValueOperation . object : Refer to #/definitions/WindowBySessionOperation . object : Refer to #/definitions/WindowByTimeOperationWithHoppingWindow . object : Refer to #/definitions/WindowByTimeOperationWithSlidingWindow . object : Refer to #/definitions/WindowByTimeOperationWithTumblingWindow . PredicateDefinition (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The expression returned by the predicate. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"predicate\"] . PredicateDefinitionWithImplicitStoreType (object) : Defines a predicate function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the predicate. Any of boolean integer number string expression : (optional) The expression returned by the predicate. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the predicate. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the predicate. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the predicate. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the predicate. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the predicate uses. Only required if the function wants to use a state store. Items (string) PrintOperation (object) : Operation to print the contents of a pipeline on the screen or to write them to a file. Cannot contain additional properties. filename (string) : (optional) The filename to output records to. If nothing is specified, then messages will be printed on stdout. label (string) : (optional) A label to attach to the output records. mapper : (optional) A function to convert record into a string for output. Any of string object : Refer to #/definitions/KeyValuePrinterDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. ProducerDefinition (object) : Definition of a Producer that regularly generates messages for a topic. Cannot contain additional properties. batchSize (integer) : (optional) The size of batches. condition : (optional) A function that validates the generator's result message. Returns \"true\" when the message may be produced on the topic, \"false\" otherwise. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . count (integer) : (optional) The number of messages to produce. generator : The function that generates records. Any of string object : Refer to #/definitions/GeneratorDefinitionWithImplicitStoreType . interval : (optional) The interval with which the generator is called. Any of integer string to : The topic to produce to. Any of string object : Refer to #/definitions/TopicDefinition . until : (optional) A predicate that returns true to indicate producing should stop. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . ReduceOperationWithAdderAndSubtractor (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. adder : A function that adds a record to the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . subtractor : A function that removes a record from the aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"reduce\"] . ReduceOperationWithReducer (object) : Operation to reduce a series of records into a single aggregate result. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. reducer : A function that computes a new aggregate result. Any of string object : Refer to #/definitions/ReducerDefinitionWithImplicitStoreType . store : (optional) Materialized view of the aggregation. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinition . object : Refer to #/definitions/SessionStateStoreDefinition . object : Refer to #/definitions/WindowStateStoreDefinition . type : The type of the operation. Must be one of: [\"reduce\"] . ReducerDefinition (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The expression returned by the reducer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"reducer\"] . ReducerDefinitionWithImplicitStoreType (object) : Defines a reducer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the reducer. Any of boolean integer number string expression : (optional) The expression returned by the reducer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the reducer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the reducer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the reducer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the reducer. Only required for function types, which are not pre-defined. RepartitionOperation (object) : Operation to (re)partition a stream. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. numberOfPartitions (integer) : (optional) The target number of partitions. partitioner : (optional) A function that partitions stream records. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"repartition\"] . SessionStateStoreDefinition (object) : Definition of a session state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the session store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the session store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this session store, \"false\" otherwise. name (string) : (optional) The name of the session store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this session store needs to be stored on disk, \"false\" otherwise. retention : (optional) The duration for which elements in the session store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"session\"] . valueType (string) : (optional) The value type of the session store. StreamDefinition (object) : Contains a definition of a Stream, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the stream. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this stream. valueType (string, required) : The value type of the stream. StreamDefinitionAsJoinTarget (object) : Reference to a Stream in a join or merge operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the stream. topic (string, required) : The name of the Kafka topic for this stream. valueType (string) : (optional) The value type of the stream. StreamPartitionerDefinition (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The expression returned by the stream partitioner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"streamPartitioner\"] . StreamPartitionerDefinitionWithImplicitStoreType (object) : Defines a stream partitioner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the stream partitioner. Any of boolean integer number string expression : (optional) The expression returned by the stream partitioner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the stream partitioner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the stream partitioner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the stream partitioner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the stream partitioner. Only required for function types, which are not pre-defined. StringOrInlinePredicateDefinitionWithImplicitStoreType (object) : Defines the condition under which messages get sent down this branch. Cannot contain additional properties. if : (optional) Defines the condition under which messages get sent down this branch. Any of string object : Refer to #/definitions/PredicateDefinitionWithImplicitStoreType . SuppressOperationUntilTimeLimit (object) : Operation to suppress messages in the source stream until a time limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . duration : The duration for which messages are suppressed. Any of integer string maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"timeLimit\"] . SuppressOperationUntilWindowCloses (object) : Operation to suppress messages in the source stream until a window limit is reached. Cannot contain additional properties. bufferFullStrategy : (optional) What to do when the buffer is full. Must be one of: [\"emitEarlyWhenFull\", \"shutdownWhenFull\"] . maxBytes (string) : (optional) The maximum number of bytes in the buffer. maxRecords (string) : (optional) The maximum number of records in the buffer. name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"suppress\"] . until : The until of the Operation to suppress messages in the source stream until a certain limit is reached. Must be one of: [\"windowCloses\"] . TableDefinition (object) : Contains a definition of a Table, which can be referenced by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the table. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic for this table. valueType (string, required) : The value type of the table. TableDefinitionAsJoinTarget (object) : Reference to a Table in a join operation. Cannot contain additional properties. keyType (string) : (optional) The key type of the table. store : (optional) KeyValue state store definition. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreTypeWithImplicitKeyAndValueType . topic (string, required) : The name of the Kafka topic for this table. valueType (string) : (optional) The value type of the table. TimestampExtractorDefinition (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The expression returned by the timestamp extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"timestampExtractor\"] . TimestampExtractorDefinitionWithImplicitStoreType (object) : Defines a timestamp extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the timestamp extractor. Any of boolean integer number string expression : (optional) The expression returned by the timestamp extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the timestamp extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the timestamp extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the timestamp extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the timestamp extractor. Only required for function types, which are not pre-defined. ToStreamOperation (object) : Convert a Table into a Stream, optionally through a custom key transformer. Cannot contain additional properties. mapper : (optional) A function that computes the output key for every record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"toStream\"] . ToTableOperation (object) : Convert a Stream into a Table. Cannot contain additional properties. name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the result table. Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"toTable\"] . ToTopicDefinition (object) : Writes out pipeline messages to a topic. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. ToTopicNameExtractorDefinition (object) : Writes out pipeline messages to a topic as given by a topic name extractor. Cannot contain additional properties. partitioner : (optional) A function that partitions the records in the output topic. Any of string object : Refer to #/definitions/StreamPartitionerDefinitionWithImplicitStoreType . topicNameExtractor : Reference to a pre-defined topic name extractor, or an inline definition of a topic name extractor. Any of string object : Refer to #/definitions/TopicNameExtractorDefinitionWithImplicitStoreType . TopicDefinition (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string) : (optional) The key type of the topic. topic (string, required) : The name of the Kafka topic. valueType (string) : (optional) The value type of the topic. TopicDefinitionSource (object) : Contains a definition of a Kafka topic, to be used by producers and pipelines. Cannot contain additional properties. keyType (string, required) : The key type of the topic. offsetResetPolicy (string) : (optional) Policy that determines what to do when there is no initial offset in Kafka, or if the current offset does not exist any more on the server (e.g. because that data has been deleted). timestampExtractor : (optional) A function extracts the event time from a consumed record. Any of string object : Refer to #/definitions/TimestampExtractorDefinitionWithImplicitStoreType . topic (string, required) : The name of the Kafka topic. valueType (string, required) : The value type of the topic. TopicNameExtractorDefinition (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The expression returned by the topic name extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"topicNameExtractor\"] . TopicNameExtractorDefinitionWithImplicitStoreType (object) : Defines a topic name extractor function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the topic name extractor. Any of boolean integer number string expression : (optional) The expression returned by the topic name extractor. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the topic name extractor. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the topic name extractor. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the topic name extractor. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the topic name extractor. Only required for function types, which are not pre-defined. TransformKeyOperation (object) : Convert the key of every record in the stream to another key. Cannot contain additional properties. mapper : A function that computes a new key for each record. Any of string object : Refer to #/definitions/KeyTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKey\", \"mapKey\", \"selectKey\"] . TransformKeyValueOperation (object) : Convert the key/value of every record in the stream to another key/value. Cannot contain additional properties. mapper : A function that computes a new key/value for each record. Any of string object : Refer to #/definitions/KeyValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"map\", \"transformKeyValue\"] . TransformKeyValueToKeyValueListOperation (object) : Convert a stream by transforming every record into a list of derived records. Cannot contain additional properties. mapper : A function that converts every record of a stream to a list of output records. Any of string object : Refer to #/definitions/KeyValueToKeyValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToKeyValueList\", \"flatMap\"] . TransformKeyValueToValueListOperation (object) : Convert every record in the stream to a list of output records with the same key. Cannot contain additional properties. mapper : A function that converts every key/value into a list of result values, each of which will be combined with the original key to form a new message in the output stream. Any of string object : Refer to #/definitions/KeyValueToValueListTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformKeyValueToValueList\", \"flatMapValues\"] . TransformMetadataOperation (object) : Convert the metadata of every record in the stream. Cannot contain additional properties. mapper : A function that converts the metadata (Kafka headers, timestamp) of every record in the stream. Any of string object : Refer to #/definitions/MetadataTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"transformMetadata\"] . TransformValueOperation (object) : Convert the value of every record in the stream to another value. Cannot contain additional properties. mapper : A function that converts the value of every record into another value. Any of string object : Refer to #/definitions/ValueTransformerDefinitionWithImplicitStoreType . name (string) : (optional) The name of the operation processor. store : (optional) Materialized view of the transformed table (only applies to tables, ignored for streams). Any of string object : Refer to #/definitions/KeyValueStateStoreDefinitionWithImplicitStoreType . type : The type of the operation. Must be one of: [\"mapValue\", \"transformValue\", \"mapValues\"] . ValueJoinerDefinition (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The expression returned by the value joiner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. type : The type of the function. Must be one of: [\"valueJoiner\"] . ValueJoinerDefinitionWithImplicitStoreType (object) : Defines a value joiner function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value joiner. Any of boolean integer number string expression : (optional) The expression returned by the value joiner. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value joiner. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value joiner. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value joiner. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value joiner. Only required for function types, which are not pre-defined. ValueTransformerDefinition (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The expression returned by the value transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) type : The type of the function. Must be one of: [\"valueTransformer\"] . ValueTransformerDefinitionWithImplicitStoreType (object) : Defines a value transformer function, that gets injected into the Kafka Streams topology. Cannot contain additional properties. code : (optional) The (multiline) code of the value transformer. Any of boolean integer number string expression : (optional) The expression returned by the value transformer. Only required for functions that return values. Any of boolean integer number string globalCode : (optional) Global (multiline) code that gets loaded into the Python context outside of the value transformer. Can be used for defining eg. global variables. Any of boolean integer number string name (string) : (optional) The name of the value transformer. If this field is not defined, then the name is derived from the context. parameters (array) : (optional) A list of parameters to be passed into the value transformer. Items (object) : Refer to #/definitions/ParameterDefinition . resultType (string) : (optional) The data type returned by the value transformer. Only required for function types, which are not pre-defined. stores (array) : (optional) A list of store names that the value transformer uses. Only required if the function wants to use a state store. Items (string) WindowBySessionOperation (object) : Operation to window messages by session, configured by an inactivity gap. Cannot contain additional properties. grace : (optional) (Tumbling, Hopping) The grace period, during which out-of-order records can still be processed. Any of integer string inactivityGap : The inactivity gap, below which two messages are considered to be of the same session. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowBySession\"] . WindowByTimeOperationWithHoppingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. advanceBy : The amount of time to increase time windows by. Any of integer string duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"hopping\"] . WindowByTimeOperationWithSlidingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. timeDifference : The maximum amount of time difference between two records. Any of integer string type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"sliding\"] . WindowByTimeOperationWithTumblingWindow (object) : Operation to window records based on time criteria. Cannot contain additional properties. duration : The duration of time windows. Any of integer string grace : (optional) The grace period, during which out-of-order records can still be processed. Any of integer string name (string) : (optional) The name of the operation processor. type : The type of the operation. Must be one of: [\"windowByTime\"] . windowType : The windowType of the time window. Must be one of: [\"tumbling\"] . WindowStateStoreDefinition (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string WindowStateStoreDefinitionWithImplicitStoreType (object) : Definition of a window state store. Cannot contain additional properties. caching (boolean) : (optional) \"true\" if changed to the window store need to be buffered and periodically released, \"false\" to emit all changes directly. keyType (string) : (optional) The key type of the window store. logging (boolean) : (optional) \"true\" if a changelog topic should be set up on Kafka for this window store, \"false\" otherwise. name (string) : (optional) The name of the window store. If this field is not defined, then the name is derived from the context. persistent (boolean) : (optional) \"true\" if this window store needs to be stored on disk, \"false\" otherwise. retainDuplicates (boolean) : (optional) Whether or not to retain duplicates. retention : (optional) The duration for which elements in the window store are retained. Any of integer string timestamped (boolean) : (optional) \"true\" if elements in the store are timestamped, \"false\" otherwise. type : The type of the state store. Must be one of: [\"window\"] . valueType (string) : (optional) The value type of the window store. windowSize : (optional) Size of the windows (cannot be negative). Any of integer string","title":"Definitions"},{"location":"quick-start/","text":"Quick start Table of Contents Introduction Starting a demo setup Starting a KSML runner Next steps Introduction KSML comes with example definitions, which contain a producer that outputs SensorData messages to Kafka, and several pipelines, which each independently consume and process the produced messages. Starting a demo setup After checking out the repository, go to the KSML directory and execute the following: docker compose up -d This will start Zookeeper, Kafka and a Schema Registry in the background. It will also start the demo producer, which outputs two random messages per second on a ksml_sensordata_avro topic. You can check the valid starting of these containers using the following command: docker compose logs -f Press CTRL-C when you verified data is produced. This typically looks like this: example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"} example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1975 example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor3, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor3\", \"owner\":\"Dave\", \"timestamp\":\"1709756689481\", \"type\":\"STATE\", \"unit\":\"state\", \"value\":\"off\"} example-producer-1 | 2024-03-06T20:24:49,483Z INFO i.a.k.r.backend.Execut^Cestamp\":1709756689814, \"type\":\"AREA\", \"unit\":\"ft2\", \"value\":\"76\"} example-producer-1 | 2024-03-06T20:24:49,815Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_xml, partition 0, offset 1129 example-producer-1 | 2024-03-06T20:24:49,921Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,922Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor6, value=SensorData: {\"city\":\"Amsterdam\", \"color\":\"yellow\", \"name\":\"sensor6\", \"owner\":\"Evan\", \"timestamp\":1709756689922, \"type\":\"AREA\", \"unit\":\"m2\", \"value\":\"245\"} example-producer-1 | 2024-03-06T20:24:49,923Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1976 example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor7, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor7\", \"owner\":\"Dave\", \"timestamp\":\"1709756690035\", \"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"0\"} Starting a KSML runner To start a container which executes the example KSML definitions, type ./examples/run.sh This will start the KSML docker container. You should see the following typical output: 2024-03-06T20:24:51,921Z INFO io.axual.ksml.runner.KSMLRunner Starting KSML Runner 1.76.0.0 ... ... ... ... 2024-03-06T20:24:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} Next steps Check out the examples in the examples directory of the project. By modifying the file examples/ksml-runner.yaml you can select the example(s) to run. For a more elaborate introduction, you can start here or refer to the documentation .","title":"Quick start"},{"location":"quick-start/#quick-start","text":"","title":"Quick start"},{"location":"quick-start/#table-of-contents","text":"Introduction Starting a demo setup Starting a KSML runner Next steps","title":"Table of Contents"},{"location":"quick-start/#introduction","text":"KSML comes with example definitions, which contain a producer that outputs SensorData messages to Kafka, and several pipelines, which each independently consume and process the produced messages.","title":"Introduction"},{"location":"quick-start/#starting-a-demo-setup","text":"After checking out the repository, go to the KSML directory and execute the following: docker compose up -d This will start Zookeeper, Kafka and a Schema Registry in the background. It will also start the demo producer, which outputs two random messages per second on a ksml_sensordata_avro topic. You can check the valid starting of these containers using the following command: docker compose logs -f Press CTRL-C when you verified data is produced. This typically looks like this: example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,480Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor2, value=SensorData: {\"city\":\"Utrecht\", \"color\":\"white\", \"name\":\"sensor2\", \"owner\":\"Alice\", \"timestamp\":1709756689480, \"type\":\"HUMIDITY\", \"unit\":\"%\", \"value\":\"66\"} example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1975 example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,481Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor3, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor3\", \"owner\":\"Dave\", \"timestamp\":\"1709756689481\", \"type\":\"STATE\", \"unit\":\"state\", \"value\":\"off\"} example-producer-1 | 2024-03-06T20:24:49,483Z INFO i.a.k.r.backend.Execut^Cestamp\":1709756689814, \"type\":\"AREA\", \"unit\":\"ft2\", \"value\":\"76\"} example-producer-1 | 2024-03-06T20:24:49,815Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_xml, partition 0, offset 1129 example-producer-1 | 2024-03-06T20:24:49,921Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:49,922Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor6, value=SensorData: {\"city\":\"Amsterdam\", \"color\":\"yellow\", \"name\":\"sensor6\", \"owner\":\"Evan\", \"timestamp\":1709756689922, \"type\":\"AREA\", \"unit\":\"m2\", \"value\":\"245\"} example-producer-1 | 2024-03-06T20:24:49,923Z INFO i.a.k.r.backend.ExecutableProducer Produced message to ksml_sensordata_avro, partition 0, offset 1976 example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.KafkaProducerRunner Calling generate_sensordata_message example-producer-1 | 2024-03-06T20:24:50,035Z INFO i.a.k.r.backend.ExecutableProducer Message: key=sensor7, value=SensorData: {\"city\":\"Alkmaar\", \"color\":\"black\", \"name\":\"sensor7\", \"owner\":\"Dave\", \"timestamp\":\"1709756690035\", \"type\":\"TEMPERATURE\", \"unit\":\"C\", \"value\":\"0\"}","title":"Starting a demo setup"},{"location":"quick-start/#starting-a-ksml-runner","text":"To start a container which executes the example KSML definitions, type ./examples/run.sh This will start the KSML docker container. You should see the following typical output: 2024-03-06T20:24:51,921Z INFO io.axual.ksml.runner.KSMLRunner Starting KSML Runner 1.76.0.0 ... ... ... ... 2024-03-06T20:24:57,196Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Alkmaar', 'color': 'yellow', 'name': 'sensor9', 'owner': 'Bob', 'timestamp': 1709749917190, 'type': 'LENGTH', 'unit': 'm', 'value': '562', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:57,631Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor3, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor3', 'owner': 'Bob', 'timestamp': 1709749917628, 'type': 'HUMIDITY', 'unit': 'g/m3', 'value': '23', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,082Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor6, value={'city': 'Amsterdam', 'color': 'white', 'name': 'sensor6', 'owner': 'Bob', 'timestamp': 1709749918078, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '64', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,528Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor9, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor9', 'owner': 'Evan', 'timestamp': 1709749918524, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '87', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:58,970Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor1, value={'city': 'Amsterdam', 'color': 'black', 'name': 'sensor1', 'owner': 'Bob', 'timestamp': 1709749918964, 'type': 'TEMPERATURE', 'unit': 'F', 'value': '75', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}} 2024-03-06T20:24:59,412Z INFO ksml.functions.log_message Consumed AVRO message - key=sensor5, value={'city': 'Amsterdam', 'color': 'blue', 'name': 'sensor5', 'owner': 'Bob', 'timestamp': 1709749919409, 'type': 'LENGTH', 'unit': 'm', 'value': '658', '@type': 'SensorData', '@schema': { <<Cleaned KSML Representation of Avro Schema>>}}","title":"Starting a KSML runner"},{"location":"quick-start/#next-steps","text":"Check out the examples in the examples directory of the project. By modifying the file examples/ksml-runner.yaml you can select the example(s) to run. For a more elaborate introduction, you can start here or refer to the documentation .","title":"Next steps"},{"location":"release-notes/","text":"Release Notes Releases Release Notes Releases 1.0.4 (2024-10-18) 1.0.2 (2024-09-20) 1.0.1 (2024-07-17) 1.0.0 (2024-06-28) 0.8.0 (2024-03-08) 0.9.1 (2024-06-21) 0.9.0 (2024-06-05) 0.2.2 (2024-01-30) 0.2.1 (2023-12-20) 0.2.0 (2023-12-07) 0.1.0 (2023-03-15) 0.0.4 (2022-12-02) 0.0.3 (2021-07-30) 0.0.2 (2021-06-28) 0.0.1 (2021-04-30) 1.0.3 (2024-10-18) KSML Fix high CPU usage Upgrade to Avro 1.11.4 to fix CVE-2024-47561 1.0.2 (2024-09-20) KSML Upgrade to Kafka Streams 3.8.0 Avro Schema Registry settings no longer required if Avro not used Add missing object in KSML Json Schema Fix serialisation and list handling issues Helm charts Use liveness and readiness and startup probes to fix state issues Fix conflicting default configuration Prometheus export and ServiceMonitor 1.0.1 (2024-07-17) Topology Optimization can be applied Runtime dependencies, like LZ4 compression support, are back in the KSML image Fix parse error messages during join Fix windowed aggregation flow errors Update windowed object support in multiple operations and functions 1.0.0 (2024-06-28) Reworked parsing logic, allowing alternatives for operations and other definitions to co-exist in the KSML language specification. This allows for better syntax checking in IDEs. Lots of small fixes and completion modifications. 0.9.1 (2024-06-21) Fix failing test in GitHub Actions during release Unified build workflows 0.9.0 (2024-06-05) Collectable metrics New topology test suite Python context hardening Improved handling of Kafka tombstones Added flexibility to producers (single shot, n-shot, or user condition-based) JSON Logging support Bumped GraalVM to 23.1.2 Bumped several dependency versions Several fixes and security updates 0.8.0 (2024-03-08) Reworked all parsing logic, to allow for exporting the JSON schema of the KSML specification: docs/specification.md is now derived from internal parser logic, guaranteeing consistency and completeness. examples/ksml.json contains the JSON schema, which can be loaded into IDEs for syntax validation and completion. Improved schema handling: Better compatibility checking between schema fields. Improved support for state stores: Update to state store typing and handling. Manual state stores can be defined and referenced in pipelines. Manual state stores are also available in Python functions. State stores can be used 'side-effect-free' (e.g. no AVRO schema registration) Python function improvements: Automatic variable assignment for state stores. Every Python function can use a Java Logger, integrating Python output with KSML log output. Type inference in situations where parameters or result types can be derived from the context. Lots of small language updates: Improve readability for store types, filter operations and windowing operations Introduction of the \"as\" operation, which allows for pipeline referencing and chaining. Better data type handling: Separation of data types and KSML core, allowing for easier addition of new data types in the future. Automatic conversion of data types, removing common pipeline failure scenarios. New implementation for CSV handling. Merged the different runners into a single runner. KSML definitions can now include both producers (data generators) and pipelines (Kafka Streams topologies). Removal of Kafka and Axual backend distinctions. Configuration file updates, allowing for running multiple definitions in a single runner (each in its own namespace). Examples updated to reflect the latest definition format. Documentation updated. 0.2.2 (2024-01-30) Changes: Fix KSML java process not stopping on exception Fix stream-stream join validation and align other checks Bump logback to 1.4.12 Fix to enable Streams optimisations to be applied to topology Fix resolving admin client issues causing warning messages 0.2.1 (2023-12-20) Changes: Fixed an issue with AVRO and field validations 0.2.0 (2023-12-07) Changes: Optimized Docker build Merged KSML Runners into one module, optimize Docker builds and workflow KSML documentation updates Docker image, GraalPy venv, install and GU commands fail Update GitHub Actions Small robustness improvements Issue #72 - Fix build failures when trying to use venv and install python packages Manual state store support, Kafka client cleanups and configuration changes Update and clean up dependencies Update documentation to use new runner configurations Update to GraalVM for JDK 21 Community 0.1.0 (2023-03-15) Changes: Added XML/SOAP support Added data generator Added Automatic Type Conversion Added Schema Support for XML, Avro, JSON, Schema Added Basic Error Handling 0.0.4 (2022-12-02) Changes: Update to kafka 3.2.3 Update to Java 17 Support multiple architectures in KSML, linux/amd64 and linux/arm64 Refactored internal typing system, plus some fixes to store operations Introduce queryable state stores Add better handling of NULL keys and values from Kafka Implement schema support Added Docker multistage build Bug fix for windowed objects Store improvements Support Liberica NIK Switch from Travis CI to GitHub workflow Build snapshot Docker image on pull request merged 0.0.3 (2021-07-30) Changes: Support for Python 3 through GraalVM improved data structuring bug fixes 0.0.2 (2021-06-28) Changes: Added JSON support, Named topology and name store supported 0.0.1 (2021-04-30) Changes: First alpha release","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#releases","text":"Release Notes Releases 1.0.4 (2024-10-18) 1.0.2 (2024-09-20) 1.0.1 (2024-07-17) 1.0.0 (2024-06-28) 0.8.0 (2024-03-08) 0.9.1 (2024-06-21) 0.9.0 (2024-06-05) 0.2.2 (2024-01-30) 0.2.1 (2023-12-20) 0.2.0 (2023-12-07) 0.1.0 (2023-03-15) 0.0.4 (2022-12-02) 0.0.3 (2021-07-30) 0.0.2 (2021-06-28) 0.0.1 (2021-04-30)","title":"Releases"},{"location":"release-notes/#103-2024-10-18","text":"KSML Fix high CPU usage Upgrade to Avro 1.11.4 to fix CVE-2024-47561","title":"1.0.3 (2024-10-18)"},{"location":"release-notes/#102-2024-09-20","text":"KSML Upgrade to Kafka Streams 3.8.0 Avro Schema Registry settings no longer required if Avro not used Add missing object in KSML Json Schema Fix serialisation and list handling issues Helm charts Use liveness and readiness and startup probes to fix state issues Fix conflicting default configuration Prometheus export and ServiceMonitor","title":"1.0.2 (2024-09-20)"},{"location":"release-notes/#101-2024-07-17","text":"Topology Optimization can be applied Runtime dependencies, like LZ4 compression support, are back in the KSML image Fix parse error messages during join Fix windowed aggregation flow errors Update windowed object support in multiple operations and functions","title":"1.0.1 (2024-07-17)"},{"location":"release-notes/#100-2024-06-28","text":"Reworked parsing logic, allowing alternatives for operations and other definitions to co-exist in the KSML language specification. This allows for better syntax checking in IDEs. Lots of small fixes and completion modifications.","title":"1.0.0 (2024-06-28)"},{"location":"release-notes/#091-2024-06-21","text":"Fix failing test in GitHub Actions during release Unified build workflows","title":"0.9.1 (2024-06-21)"},{"location":"release-notes/#090-2024-06-05","text":"Collectable metrics New topology test suite Python context hardening Improved handling of Kafka tombstones Added flexibility to producers (single shot, n-shot, or user condition-based) JSON Logging support Bumped GraalVM to 23.1.2 Bumped several dependency versions Several fixes and security updates","title":"0.9.0 (2024-06-05)"},{"location":"release-notes/#080-2024-03-08","text":"Reworked all parsing logic, to allow for exporting the JSON schema of the KSML specification: docs/specification.md is now derived from internal parser logic, guaranteeing consistency and completeness. examples/ksml.json contains the JSON schema, which can be loaded into IDEs for syntax validation and completion. Improved schema handling: Better compatibility checking between schema fields. Improved support for state stores: Update to state store typing and handling. Manual state stores can be defined and referenced in pipelines. Manual state stores are also available in Python functions. State stores can be used 'side-effect-free' (e.g. no AVRO schema registration) Python function improvements: Automatic variable assignment for state stores. Every Python function can use a Java Logger, integrating Python output with KSML log output. Type inference in situations where parameters or result types can be derived from the context. Lots of small language updates: Improve readability for store types, filter operations and windowing operations Introduction of the \"as\" operation, which allows for pipeline referencing and chaining. Better data type handling: Separation of data types and KSML core, allowing for easier addition of new data types in the future. Automatic conversion of data types, removing common pipeline failure scenarios. New implementation for CSV handling. Merged the different runners into a single runner. KSML definitions can now include both producers (data generators) and pipelines (Kafka Streams topologies). Removal of Kafka and Axual backend distinctions. Configuration file updates, allowing for running multiple definitions in a single runner (each in its own namespace). Examples updated to reflect the latest definition format. Documentation updated.","title":"0.8.0 (2024-03-08)"},{"location":"release-notes/#022-2024-01-30","text":"Changes: Fix KSML java process not stopping on exception Fix stream-stream join validation and align other checks Bump logback to 1.4.12 Fix to enable Streams optimisations to be applied to topology Fix resolving admin client issues causing warning messages","title":"0.2.2 (2024-01-30)"},{"location":"release-notes/#021-2023-12-20","text":"Changes: Fixed an issue with AVRO and field validations","title":"0.2.1 (2023-12-20)"},{"location":"release-notes/#020-2023-12-07","text":"Changes: Optimized Docker build Merged KSML Runners into one module, optimize Docker builds and workflow KSML documentation updates Docker image, GraalPy venv, install and GU commands fail Update GitHub Actions Small robustness improvements Issue #72 - Fix build failures when trying to use venv and install python packages Manual state store support, Kafka client cleanups and configuration changes Update and clean up dependencies Update documentation to use new runner configurations Update to GraalVM for JDK 21 Community","title":"0.2.0 (2023-12-07)"},{"location":"release-notes/#010-2023-03-15","text":"Changes: Added XML/SOAP support Added data generator Added Automatic Type Conversion Added Schema Support for XML, Avro, JSON, Schema Added Basic Error Handling","title":"0.1.0 (2023-03-15)"},{"location":"release-notes/#004-2022-12-02","text":"Changes: Update to kafka 3.2.3 Update to Java 17 Support multiple architectures in KSML, linux/amd64 and linux/arm64 Refactored internal typing system, plus some fixes to store operations Introduce queryable state stores Add better handling of NULL keys and values from Kafka Implement schema support Added Docker multistage build Bug fix for windowed objects Store improvements Support Liberica NIK Switch from Travis CI to GitHub workflow Build snapshot Docker image on pull request merged","title":"0.0.4 (2022-12-02)"},{"location":"release-notes/#003-2021-07-30","text":"Changes: Support for Python 3 through GraalVM improved data structuring bug fixes","title":"0.0.3 (2021-07-30)"},{"location":"release-notes/#002-2021-06-28","text":"Changes: Added JSON support, Named topology and name store supported","title":"0.0.2 (2021-06-28)"},{"location":"release-notes/#001-2021-04-30","text":"Changes: First alpha release","title":"0.0.1 (2021-04-30)"},{"location":"reference-docs/","text":"KSML Reference docs Runners Pipelines Streams Types Notations Operations Functions Stores","title":"KSML Reference docs"},{"location":"reference-docs/#ksml-reference-docs","text":"Runners Pipelines Streams Types Notations Operations Functions Stores","title":"KSML Reference docs"},{"location":"reference-docs/functions/","text":"Functions Table of Contents Introduction Data types in Python Data type mapping Automatic conversion Function Types Function parameters Logger Metrics State stores Introduction Functions can be specified in the functions section of a KSML definition file. The layout typically looks like this: functions: my_first_predicate: type: predicate expression: key=='Some string' compare_params: type: generic parameters: - name: firstParam type: string - name: secondParam type: int globalCode: | import something from package globalVar = 3 code: | print('Hello there!') expression: firstParam == str(secondParam) Functions are defined by the following tags: Parameter Value Type Default Description type string generic The type of the function defined parameters List of parameter definitions empty list A list of parameters, each of which contains the mandatory fields name and type . See example above. globalCode string empty Snippet of Python code that is executed once upon creation of the Kafka Streams topology. This section can contain statements like import to import function libraries used in the code and expression sections. code string empty Python source code, which will be included in the called function. expression string empty Python expression that contains the returned function result. See below for the list of supported function types. Data types in Python Internally, KSML uses an abstraction to deal with all kinds of data types. See types for more information on data types. Data type mapping Data types are automatically converted to/from Python in the following manner: Data type Python type Example boolean bool True, False bytes bytearray double float 3.145 float float 1.23456 byte int between -128 and 127 short int between -65,536 and 65,535 int int between -2,147,483,648 and 2,147,483,647 long int between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 string str \"text\" enum str enum string literal, eg. \"BLUE\", \"EUROPE\" list array [ \"key1\", \"key2\" ] struct dict { \"key1\": \"value1\", \"key2\": \"value2\" } struct with schema dict { \"key1\": \"value1\", \"key2\": \"value2\", \"@type\": \"SensorData\", \"@schema\": \"...\" } tuple tuple (1, \"text\", 3.14, { \"key\": \"value\" }) union Real value is translated as specified in this table Automatic conversion KSML is able to automatically convert between types. Examples are: To/from string conversion is handled automatically for almost all data types, including string-notations such as CSV, JSON and XML. When a string is expected, but a struct is passed in, the struct is automatically converted to string. When a struct is expected, but a string is passed in, the string is parsed according to the notation specified. Field matching and field type conversion is done automatically. For instance, if a struct contains an integer field, but the target schema expects a string, the integer is automatically converted to string. Function Types Functions in KSML always have a type . When no type is specified, the function type is inferred from the context, or it defaults back to generic . This section discusses the purpose of every function type, and what fixed arguments every call gets passed in. Aggregator An aggregator incrementally integrates a new keu/value into an aggregatedValue. It is called for every new message that becomes part of the aggregated result. The following highlights which calls are made to which function type during a regular aggregation, in this case for counting the number of messages: # Aggregation starts initializer() -> 0 msg1: aggregator(msg1.key, msg1.value, 0) -> 1 msg2: aggregator(msg2.key, msg2.value, 1) -> 1 msg3: aggregator(msg3.key, msg3.value, 2) -> 3 The result in this example is 3. Aggregators get the following fixed arguments: Parameter Value Type Description key any The key of the message to be included in the aggregated result thus far. value any The value of the message to be included in the aggregated result thus far. aggregatedValue any The aggregated value thus far. returns any The new aggregated result, which includes the latest message. ForEach A forEach function is called for every message in a stream. When part of a forEach operation at the end of a pipeline, the function is the last one called for every message. When this function is called during peek operations, it may look at the messages and cause side effects (e.g. printing the message to stdout), and the pipeline will continue with the unmodified message after doing so. ForEach functions get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns none Nothing is returned. ForeignKeyExtractor A foreignKeyExtractor is a function used during (left) joins of two tables. The function translates a value from \"this table\" and translates it into a key of the \"other table\" that is joined with. ForEach functions get the following fixed arguments: Parameter Value Type Description value any The value of the message. returns any The key looked up in the table joined with. Generator A generator is a function that generates new messages out of thin air. It is most often used to generate mock data for testing purposes. Generators get no arguments, and return messages to be sent to the output stream. Parameter Value Type Description returns (_any_, _any ) The key/value of the message to be sent to the output stream. Generic A generic function can be used for generic purposes. It can be used for any operation, as long as its parameters match the expected types of the operation's function. Generic functions get any arguments, and may return anything. Parameter Value Type Description self-defined any Self-defined parameters can be passed in. returns any Can return any value. Initializer An initializer is called upon the start of every (part of an) aggregation. It takes no arguments and should return an initial value for the aggregation. Parameter Value Type Description returns any An initial value for the aggregation. In a counting aggregation, this would be 0 . KeyTransformer A keyTransformer is able to transform a key/value into a new key, which then gets combined with the original value as a new message on the output stream. KeyTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The key of the output message. KeyValuePrinter A keyValuePrinter takes a message and converts it to string before outputting it to a file or printing it to stdout. KeyValuePrinters get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The string to be written to file or stdout. KeyValueToKeyValueListTransformer A keyValueToKeyValueListTransformer takes one message and converts it into a list of output messages, which then get sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToKeyValueList operation, this message can be converted into individual messages (k,item1), (k,item2), ... on the output stream. KeyValueToKeyValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [(_any_, _any_)] A list of messages for the output stream. KeyValueToValueListTransformer A keyValueToValueListTransformer takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToValueList operation, this message can be converted into a list of values [item1, item2, ...] which get combined with the key of the message into (k,item1), (k,item2), ... on the output stream. KeyValueToValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [_any_] A list of values to be combined with the key on the output stream. KeyValueTransformer A keyValueTransformer takes one message and converts it into another message, which may have different key/value types. KeyValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns (_any_, _any_) The transformed message. Merger A merger takes a key and two values, and merges those values together into a new value. That value is combined with the original key and sent to the output stream. Mergers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The merged value of the output message. MetadataTransformer A metadataTransformer can transform a message's metadata (headers and timestamp). MetadataTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. metadata dict Contains the headers and timestamp of the message. returns dict The (optionally) modified metadata for the output message. This structure should have the same type as the metadata passed in. Predicate A predicate is a function that takes the key/value of a message and returns True or False . It is used for filtering and branching purposes (e.g. routing messages based on content). Predicates get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns boolean True or False . Reducer A reducer is a function that combines two aggregated results into one. Reducers get the following fixed arguments: Parameter Value Type Description value1 any The value of the first aggregation result. value2 any The value of the second aggregation result. returns any The value of the combined aggregation result. StreamPartitioner A streamPartitioner is a function that can assign a partition number to every message. It is used to repartition Kafka topics, based on message contents. StreamPartitioners get the following fixed arguments: Parameter Value Type Description topic string The topic of the message. key any The key of the message. value any The value of the message. numPartitions integer The number of partitions available on the output topic. returns integer The partition number to which this message gets sent. TimestampExtractor A timestampExtractor is a function which can determine a timestamp from a given input message, which is used for all downstream processing. TimestampExtractors get the following fixed arguments: Parameter Value Type Description record struct A dictionary containing the timestamp , timestampType , key , value , topic , partition and offset of the input message. previousTimestamp long The timestamp of the last message (before this one). returns long The timestamp to apply to this message. TopicNameExtractor A topicNameExtractor is a function which can derive a topic name from a message, for example by getting the customer name from a message and deriving the topic name from that. It is used by toTopicNameExtractor operations to send messages to individually determined topics. TopicNameExtractors get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The name of the topic to send this message to. ValueJoiner A valueJoiner takes a key and two values, and combines the two values into one. That value is then combined with the original key and sent to the output stream. ValueJoiners get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The joined value of the output message. ValueTransformer A valueTransformer takes a key/value and transforms it into a new value, which is combined with the original key and sent to the output stream. ValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The value of the output message. Function parameters Besides the parameters mentioned above, all Python functions in KSML get special parameters passed in: Logger Every function can access the log variable, which is mapped to a plain Java Logger object. It can be used to send output to the KSML log by calling its methods. It supports the following operations: Method Description error(message: str, value_params...) logs an error message warn(message: str, value_params...) logs a warning message info(message: str, value_params...) logs an informational message debug(message: str, value_params...) logs a debug message trace(message: str, value_params...) logs a trace message The message contains double curly brackets {} , which will be substituted by the value parameters. Examples are: log.error(\"Something went completely bad here!\") log.info(\"Received message from topic: key={}, value={}\", key, value) log.debug(\"I'm printing five variables here: {}, {}, {}, {}, {}. Lovely isn't it?\", 1, 2, 3, \"text\", {\"json\":\"is cool\"}) Output of the above statements looks like: [LOG TIMESTAMP] ERROR function.name Something went completely bad here! [LOG TIMESTAMP] INFO function.name Received message from topic: key=123, value={\"key\":\"value\"} [LOG TIMESTAMP] DEBUG function.name I'm printing five variables here: 1, 2, 3, text, {\"json\":\"is cool\"}. Lovely isn't it? Metrics KSML supports metric collection and exposure through JMX and built-in Prometheus agent. Metrics for Python functions are automatically generated and collected, but users can also specify their own metrics. For an example, see 17-example-inspect-with-metrics.yaml in the examples directory. KSML supports the following metric types: Counter: an increasing integer, which counts for example the number of calls made to a Python function. Meter: used for periodically updating a measurement value. Preferred over Counter when don't care too much about exact averages, but want to monitor trends instead. Timer: measures the time spent by processes or functions, that get called internally. Every Python function in KSML can use the metrics variable, which is made available by KSML. The object supports the following methods to create your own metrics: counter(name: str, tags: dict) -> Counter counter(name: str) -> Counter meter(name: str, tags: dict) -> Meter meter(name: str) -> Meter timer(name: str, tags: dict) -> Timer timer(name: str) -> Timer In turn these objects support the following: Counter increment() increment(delta: int) Meter mark() mark(nrOfEvents: int) Timer updateSeconds(valueSeconds: int) updateMillis(valueMillis: int) updateNanos(valueNanos: int) State stores Some functions are allowed to access local state stores. These functions specify the stores attribute in their definitions. The state stores they reference are accessible as variables with the same name as the state store. Examples: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData stores: last_sensor_data_store: type: keyValue keyType: string valueType: json persistent: false historyRetention: 1h caching: false logging: false functions: process_message: type: forEach code: | last_value = last_sensor_data_store.get(key) if last_value is not None: log.info(\"Found last value: {} = {}\", key, last_value) last_sensor_data_store.put(key, value) if value is not None: log.info(\"Stored new value: {} = {}\", key, value) stores: - last_sensor_data_store pipelines: process_message: from: sensor_source_avro forEach: process_message In this example the function process_message uses the state store last_sensor_data_store directly as a variable. It is allowed to do that when it declares such use in its definition under the stores attribute. State stores have common methods like get and put , which you can call directly from Python code.","title":"Functions"},{"location":"reference-docs/functions/#functions","text":"","title":"Functions"},{"location":"reference-docs/functions/#table-of-contents","text":"Introduction Data types in Python Data type mapping Automatic conversion Function Types Function parameters Logger Metrics State stores","title":"Table of Contents"},{"location":"reference-docs/functions/#introduction","text":"Functions can be specified in the functions section of a KSML definition file. The layout typically looks like this: functions: my_first_predicate: type: predicate expression: key=='Some string' compare_params: type: generic parameters: - name: firstParam type: string - name: secondParam type: int globalCode: | import something from package globalVar = 3 code: | print('Hello there!') expression: firstParam == str(secondParam) Functions are defined by the following tags: Parameter Value Type Default Description type string generic The type of the function defined parameters List of parameter definitions empty list A list of parameters, each of which contains the mandatory fields name and type . See example above. globalCode string empty Snippet of Python code that is executed once upon creation of the Kafka Streams topology. This section can contain statements like import to import function libraries used in the code and expression sections. code string empty Python source code, which will be included in the called function. expression string empty Python expression that contains the returned function result. See below for the list of supported function types.","title":"Introduction"},{"location":"reference-docs/functions/#data-types-in-python","text":"Internally, KSML uses an abstraction to deal with all kinds of data types. See types for more information on data types.","title":"Data types in Python"},{"location":"reference-docs/functions/#data-type-mapping","text":"Data types are automatically converted to/from Python in the following manner: Data type Python type Example boolean bool True, False bytes bytearray double float 3.145 float float 1.23456 byte int between -128 and 127 short int between -65,536 and 65,535 int int between -2,147,483,648 and 2,147,483,647 long int between -9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 string str \"text\" enum str enum string literal, eg. \"BLUE\", \"EUROPE\" list array [ \"key1\", \"key2\" ] struct dict { \"key1\": \"value1\", \"key2\": \"value2\" } struct with schema dict { \"key1\": \"value1\", \"key2\": \"value2\", \"@type\": \"SensorData\", \"@schema\": \"...\" } tuple tuple (1, \"text\", 3.14, { \"key\": \"value\" }) union Real value is translated as specified in this table","title":"Data type mapping"},{"location":"reference-docs/functions/#automatic-conversion","text":"KSML is able to automatically convert between types. Examples are: To/from string conversion is handled automatically for almost all data types, including string-notations such as CSV, JSON and XML. When a string is expected, but a struct is passed in, the struct is automatically converted to string. When a struct is expected, but a string is passed in, the string is parsed according to the notation specified. Field matching and field type conversion is done automatically. For instance, if a struct contains an integer field, but the target schema expects a string, the integer is automatically converted to string.","title":"Automatic conversion"},{"location":"reference-docs/functions/#function-types","text":"Functions in KSML always have a type . When no type is specified, the function type is inferred from the context, or it defaults back to generic . This section discusses the purpose of every function type, and what fixed arguments every call gets passed in.","title":"Function Types"},{"location":"reference-docs/functions/#aggregator","text":"An aggregator incrementally integrates a new keu/value into an aggregatedValue. It is called for every new message that becomes part of the aggregated result. The following highlights which calls are made to which function type during a regular aggregation, in this case for counting the number of messages: # Aggregation starts initializer() -> 0 msg1: aggregator(msg1.key, msg1.value, 0) -> 1 msg2: aggregator(msg2.key, msg2.value, 1) -> 1 msg3: aggregator(msg3.key, msg3.value, 2) -> 3 The result in this example is 3. Aggregators get the following fixed arguments: Parameter Value Type Description key any The key of the message to be included in the aggregated result thus far. value any The value of the message to be included in the aggregated result thus far. aggregatedValue any The aggregated value thus far. returns any The new aggregated result, which includes the latest message.","title":"Aggregator"},{"location":"reference-docs/functions/#foreach","text":"A forEach function is called for every message in a stream. When part of a forEach operation at the end of a pipeline, the function is the last one called for every message. When this function is called during peek operations, it may look at the messages and cause side effects (e.g. printing the message to stdout), and the pipeline will continue with the unmodified message after doing so. ForEach functions get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns none Nothing is returned.","title":"ForEach"},{"location":"reference-docs/functions/#foreignkeyextractor","text":"A foreignKeyExtractor is a function used during (left) joins of two tables. The function translates a value from \"this table\" and translates it into a key of the \"other table\" that is joined with. ForEach functions get the following fixed arguments: Parameter Value Type Description value any The value of the message. returns any The key looked up in the table joined with.","title":"ForeignKeyExtractor"},{"location":"reference-docs/functions/#generator","text":"A generator is a function that generates new messages out of thin air. It is most often used to generate mock data for testing purposes. Generators get no arguments, and return messages to be sent to the output stream. Parameter Value Type Description returns (_any_, _any ) The key/value of the message to be sent to the output stream.","title":"Generator"},{"location":"reference-docs/functions/#generic","text":"A generic function can be used for generic purposes. It can be used for any operation, as long as its parameters match the expected types of the operation's function. Generic functions get any arguments, and may return anything. Parameter Value Type Description self-defined any Self-defined parameters can be passed in. returns any Can return any value.","title":"Generic"},{"location":"reference-docs/functions/#initializer","text":"An initializer is called upon the start of every (part of an) aggregation. It takes no arguments and should return an initial value for the aggregation. Parameter Value Type Description returns any An initial value for the aggregation. In a counting aggregation, this would be 0 .","title":"Initializer"},{"location":"reference-docs/functions/#keytransformer","text":"A keyTransformer is able to transform a key/value into a new key, which then gets combined with the original value as a new message on the output stream. KeyTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The key of the output message.","title":"KeyTransformer"},{"location":"reference-docs/functions/#keyvalueprinter","text":"A keyValuePrinter takes a message and converts it to string before outputting it to a file or printing it to stdout. KeyValuePrinters get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The string to be written to file or stdout.","title":"KeyValuePrinter"},{"location":"reference-docs/functions/#keyvaluetokeyvaluelisttransformer","text":"A keyValueToKeyValueListTransformer takes one message and converts it into a list of output messages, which then get sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToKeyValueList operation, this message can be converted into individual messages (k,item1), (k,item2), ... on the output stream. KeyValueToKeyValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [(_any_, _any_)] A list of messages for the output stream.","title":"KeyValueToKeyValueListTransformer"},{"location":"reference-docs/functions/#keyvaluetovaluelisttransformer","text":"A keyValueToValueListTransformer takes one message and converts it into a list of output values, which then get combined with the original key and sent to the output stream. An example for this type of function would be a message, which contains a list of items in its value (e.g. (k, [item]) . Using a transformKeyValueToValueList operation, this message can be converted into a list of values [item1, item2, ...] which get combined with the key of the message into (k,item1), (k,item2), ... on the output stream. KeyValueToValueListTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns [_any_] A list of values to be combined with the key on the output stream.","title":"KeyValueToValueListTransformer"},{"location":"reference-docs/functions/#keyvaluetransformer","text":"A keyValueTransformer takes one message and converts it into another message, which may have different key/value types. KeyValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns (_any_, _any_) The transformed message.","title":"KeyValueTransformer"},{"location":"reference-docs/functions/#merger","text":"A merger takes a key and two values, and merges those values together into a new value. That value is combined with the original key and sent to the output stream. Mergers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The merged value of the output message.","title":"Merger"},{"location":"reference-docs/functions/#metadatatransformer","text":"A metadataTransformer can transform a message's metadata (headers and timestamp). MetadataTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. metadata dict Contains the headers and timestamp of the message. returns dict The (optionally) modified metadata for the output message. This structure should have the same type as the metadata passed in.","title":"MetadataTransformer"},{"location":"reference-docs/functions/#predicate","text":"A predicate is a function that takes the key/value of a message and returns True or False . It is used for filtering and branching purposes (e.g. routing messages based on content). Predicates get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns boolean True or False .","title":"Predicate"},{"location":"reference-docs/functions/#reducer","text":"A reducer is a function that combines two aggregated results into one. Reducers get the following fixed arguments: Parameter Value Type Description value1 any The value of the first aggregation result. value2 any The value of the second aggregation result. returns any The value of the combined aggregation result.","title":"Reducer"},{"location":"reference-docs/functions/#streampartitioner","text":"A streamPartitioner is a function that can assign a partition number to every message. It is used to repartition Kafka topics, based on message contents. StreamPartitioners get the following fixed arguments: Parameter Value Type Description topic string The topic of the message. key any The key of the message. value any The value of the message. numPartitions integer The number of partitions available on the output topic. returns integer The partition number to which this message gets sent.","title":"StreamPartitioner"},{"location":"reference-docs/functions/#timestampextractor","text":"A timestampExtractor is a function which can determine a timestamp from a given input message, which is used for all downstream processing. TimestampExtractors get the following fixed arguments: Parameter Value Type Description record struct A dictionary containing the timestamp , timestampType , key , value , topic , partition and offset of the input message. previousTimestamp long The timestamp of the last message (before this one). returns long The timestamp to apply to this message.","title":"TimestampExtractor"},{"location":"reference-docs/functions/#topicnameextractor","text":"A topicNameExtractor is a function which can derive a topic name from a message, for example by getting the customer name from a message and deriving the topic name from that. It is used by toTopicNameExtractor operations to send messages to individually determined topics. TopicNameExtractors get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns string The name of the topic to send this message to.","title":"TopicNameExtractor"},{"location":"reference-docs/functions/#valuejoiner","text":"A valueJoiner takes a key and two values, and combines the two values into one. That value is then combined with the original key and sent to the output stream. ValueJoiners get the following fixed arguments: Parameter Value Type Description key any The key of the message. value1 any The value of the first message. value2 any The value of the second message. returns any The joined value of the output message.","title":"ValueJoiner"},{"location":"reference-docs/functions/#valuetransformer","text":"A valueTransformer takes a key/value and transforms it into a new value, which is combined with the original key and sent to the output stream. ValueTransformers get the following fixed arguments: Parameter Value Type Description key any The key of the message. value any The value of the message. returns any The value of the output message.","title":"ValueTransformer"},{"location":"reference-docs/functions/#function-parameters","text":"Besides the parameters mentioned above, all Python functions in KSML get special parameters passed in:","title":"Function parameters"},{"location":"reference-docs/functions/#logger","text":"Every function can access the log variable, which is mapped to a plain Java Logger object. It can be used to send output to the KSML log by calling its methods. It supports the following operations: Method Description error(message: str, value_params...) logs an error message warn(message: str, value_params...) logs a warning message info(message: str, value_params...) logs an informational message debug(message: str, value_params...) logs a debug message trace(message: str, value_params...) logs a trace message The message contains double curly brackets {} , which will be substituted by the value parameters. Examples are: log.error(\"Something went completely bad here!\") log.info(\"Received message from topic: key={}, value={}\", key, value) log.debug(\"I'm printing five variables here: {}, {}, {}, {}, {}. Lovely isn't it?\", 1, 2, 3, \"text\", {\"json\":\"is cool\"}) Output of the above statements looks like: [LOG TIMESTAMP] ERROR function.name Something went completely bad here! [LOG TIMESTAMP] INFO function.name Received message from topic: key=123, value={\"key\":\"value\"} [LOG TIMESTAMP] DEBUG function.name I'm printing five variables here: 1, 2, 3, text, {\"json\":\"is cool\"}. Lovely isn't it?","title":"Logger"},{"location":"reference-docs/functions/#metrics","text":"KSML supports metric collection and exposure through JMX and built-in Prometheus agent. Metrics for Python functions are automatically generated and collected, but users can also specify their own metrics. For an example, see 17-example-inspect-with-metrics.yaml in the examples directory. KSML supports the following metric types: Counter: an increasing integer, which counts for example the number of calls made to a Python function. Meter: used for periodically updating a measurement value. Preferred over Counter when don't care too much about exact averages, but want to monitor trends instead. Timer: measures the time spent by processes or functions, that get called internally. Every Python function in KSML can use the metrics variable, which is made available by KSML. The object supports the following methods to create your own metrics: counter(name: str, tags: dict) -> Counter counter(name: str) -> Counter meter(name: str, tags: dict) -> Meter meter(name: str) -> Meter timer(name: str, tags: dict) -> Timer timer(name: str) -> Timer In turn these objects support the following:","title":"Metrics"},{"location":"reference-docs/functions/#counter","text":"increment() increment(delta: int)","title":"Counter"},{"location":"reference-docs/functions/#meter","text":"mark() mark(nrOfEvents: int)","title":"Meter"},{"location":"reference-docs/functions/#timer","text":"updateSeconds(valueSeconds: int) updateMillis(valueMillis: int) updateNanos(valueNanos: int)","title":"Timer"},{"location":"reference-docs/functions/#state-stores","text":"Some functions are allowed to access local state stores. These functions specify the stores attribute in their definitions. The state stores they reference are accessible as variables with the same name as the state store. Examples: streams: sensor_source_avro: topic: ksml_sensordata_avro keyType: string valueType: avro:SensorData stores: last_sensor_data_store: type: keyValue keyType: string valueType: json persistent: false historyRetention: 1h caching: false logging: false functions: process_message: type: forEach code: | last_value = last_sensor_data_store.get(key) if last_value is not None: log.info(\"Found last value: {} = {}\", key, last_value) last_sensor_data_store.put(key, value) if value is not None: log.info(\"Stored new value: {} = {}\", key, value) stores: - last_sensor_data_store pipelines: process_message: from: sensor_source_avro forEach: process_message In this example the function process_message uses the state store last_sensor_data_store directly as a variable. It is allowed to do that when it declares such use in its definition under the stores attribute. State stores have common methods like get and put , which you can call directly from Python code.","title":"State stores"},{"location":"reference-docs/notations/","text":"Notations Table of Contents Introduction Avro CSV JSON SOAP XML Introduction KSML is able to express its internal data types in a number of external representations. Internally these are called notations . The different notations are described below. AVRO Avro types are supported through the \"avro\" prefix in types. The notation is avro:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, Avro types are serialized in binary format. Internally they are represented as structs. Examples: avro:SensorData avro:io.axual.ksml.example.SensorData Note: when referencing an AVRO schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .avsc file extension. CSV Comma-separated values are supported through the \"csv\" prefix in types. The notation is csv:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, CSV types are serialized as string . Internally they are represented as structs. Examples: csv:SensorData csv:io.axual.ksml.example.SensorData Note: when referencing an CSV schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .csv file extension. JSON JSON types are supported through the \"json\" prefix in types. The notation is json:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, JSON types are serialized as string . Internally they are represented as structs or lists. Examples: json:SensorData json:io.axual.ksml.example.SensorData If you want to use JSON without a schema, you can leave out the colon and schema name: json Note: when referencing an JSON schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .json file extension. SOAP SOAP is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally SOAP objects are structs with their own schema. Field names are derived from the SOAP standards. XML XML is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally XML objects are structs. Examples: xml:SensorData xml:io.axual.ksml.example.SensorData Note: when referencing an XML schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .xsd file extension.","title":"Notations"},{"location":"reference-docs/notations/#notations","text":"","title":"Notations"},{"location":"reference-docs/notations/#table-of-contents","text":"Introduction Avro CSV JSON SOAP XML","title":"Table of Contents"},{"location":"reference-docs/notations/#introduction","text":"KSML is able to express its internal data types in a number of external representations. Internally these are called notations . The different notations are described below.","title":"Introduction"},{"location":"reference-docs/notations/#avro","text":"Avro types are supported through the \"avro\" prefix in types. The notation is avro:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, Avro types are serialized in binary format. Internally they are represented as structs. Examples: avro:SensorData avro:io.axual.ksml.example.SensorData Note: when referencing an AVRO schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .avsc file extension.","title":"AVRO"},{"location":"reference-docs/notations/#csv","text":"Comma-separated values are supported through the \"csv\" prefix in types. The notation is csv:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, CSV types are serialized as string . Internally they are represented as structs. Examples: csv:SensorData csv:io.axual.ksml.example.SensorData Note: when referencing an CSV schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .csv file extension.","title":"CSV"},{"location":"reference-docs/notations/#json","text":"JSON types are supported through the \"json\" prefix in types. The notation is json:schema , where schema is the schema fqdn, or just the schema name itself. On Kafka topics, JSON types are serialized as string . Internally they are represented as structs or lists. Examples: json:SensorData json:io.axual.ksml.example.SensorData If you want to use JSON without a schema, you can leave out the colon and schema name: json Note: when referencing an JSON schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .json file extension.","title":"JSON"},{"location":"reference-docs/notations/#soap","text":"SOAP is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally SOAP objects are structs with their own schema. Field names are derived from the SOAP standards.","title":"SOAP"},{"location":"reference-docs/notations/#xml","text":"XML is supported through built-in serializers and deserializers. The representation on Kafka will always be string . Internally XML objects are structs. Examples: xml:SensorData xml:io.axual.ksml.example.SensorData Note: when referencing an XML schema, you have to ensure that the respective schema file can be found in the KSML working directory and has the .xsd file extension.","title":"XML"},{"location":"reference-docs/operations/","text":"Operations Table of Contents Introduction Operations aggregate cogroup convertKey convertKeyValue convertValue count filter filterNot flatMap flatMapValues groupBy groupByKey join leftJoin map mapKey mapValue mapValues merge outerJoin peek reduce repartition selectKey suppress toStream transformKey transformKeyValue transformKeyValueToKeyValueList transformKeyValueToValueList transformValue windowBySession windowByTime Sink Operations as branch forEach print to toTopicNameExtractor Introduction Pipelines in KSML have a beginning, a middle and (optionally) an end. Operations form the middle part of pipelines. They are modeled as separate YAML entities, where each operation takes input from the previous operation and applies its own logic. The returned stream then serves as input for the next operation. Transform Operations Transformations are operations that take an input stream and convert it to an output stream. This section lists all supported transformations. Each one states the type of stream it returns. Parameter Value Type Description name string The name of the operation. Note that not all combinations of output/input streams are supported by Kafka Streams. The user that writes the KSML definition needs to make sure that streams that result from one operation can actually serve as input to the next. KSML does type checking and will exit with an error when operations that can not be chained together are listed after another in the KSML definition. aggregate This operation aggregates multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . KGroupedTable <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . adder Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . subtractor Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type VR . SessionWindowedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . SessionWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . TimeWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream cogroup This operation cogroups multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> [CogroupedKStream] <K,VR> aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> n/a n/a n/a n/a This method is currently not supported in KSML. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: cogroup aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Note: this operation was added to KSML for completion purposes, but is not considered ready or fully functional. Feel free to experiment, but don't rely on this in production. Syntax changes may occur in future KSML releases. convertKey This built-in operation takes a message and converts the key into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,V> into string The type to convert the key into. Conversion to KR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKey into: json to: output_stream convertKeyValue This built-in operation takes a message and converts the key and value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,VR> into string The type to convert the key and value into. Conversion of key into KR and value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKeyValue into: (json,xml) to: output_stream convertValue This built-in operation takes a message and converts the value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> into string The type to convert the value into. Conversion of value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertValue into: xml to: output_stream count This operation counts the number of messages and returns a table multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . KGroupedTable <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . SessionWindowedKStream <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type session . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type window . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: count - type: toStream to: output_stream filter Filter all incoming messages according to some predicate. The predicate function is called for every message. Only when the predicate returns true , then the message will be sent to the output stream. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. KTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. Example: from: input_stream via: - type: filter if: expression: key.startswith('a') to: output_stream filterNot This operation works exactly like filter , but negates all predicates before applying them. That means messages for which the predicate returns False are accepted, while those that the predicate returns True for are filtered out. See filter for details on how to implement. flatMap This operation takes a message and transforms it into zero, one or more new messages, which may have different key and value types than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [(KR,VR)] containing a list of transformed key and value pairs. Example: from: input_stream via: - type: flatMap mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream flatMapValues This operation takes a message and transforms it into zero, one or more new values, which may have different value types than the source. Every entry in the result list is combined with the source key and produced on the output stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> mapper Inline or reference A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [VR] containing a list of transformed value s. Example: from: input_stream via: - type: flatMapValues mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream groupBy Group the records of a stream by value resulting from a KeyValueMapper. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. KTable <K,V> KGroupedTable <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. Example: from: input_stream via: - type: groupBy mapper: expression: value[\"some_field\"] resultType: string - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream groupByKey Group the records of a stream by the stream's key. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . Example: from: input_stream via: - type: groupByKey - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream join Join records of this stream with another stream's records using inner join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: join stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream leftJoin Join records of this stream with another stream's records using left join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: leftJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream map This operation takes a message and transforms the key and value into a new key and value, which can each have a different type than the source message key and value. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a tuple of type (KR,VR) containing the transformed key and value . Example: from: input_stream via: - type: map mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream mapKey This is an alias for selectKey . Example: from: input_stream via: - type: mapKey mapper: expression: str(key) # convert key from source type to string to: output_stream mapValue This is an alias for mapValues . Example: from: input_stream via: - type: mapValue mapper: expression: str(value) # convert value from source type to String to: output_stream mapValues This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a value of type VR . Example: from: input_stream via: - type: mapValues mapper: expression: str(value) # convert value from source type to String to: output_stream merge Merge this stream and the given stream into one larger stream. There is no ordering guarantee between records from this stream and records from the provided stream in the merged stream. Relative order is preserved within each input stream though (ie, records within one input stream are processed in order). Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> stream string The name of the stream to merge with. Example: from: input_stream via: - type: merge stream: second_stream to: output_stream outerJoin Join records of this stream with another stream's records using outer join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KTable <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . Example: from: input_stream via: - type: outerJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream peek Perform an action on each record of a stream. This is a stateless record-by-record operation. Peek is a non-terminal operation that triggers a side effect (such as logging or statistics collection) and returns an unchanged stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> forEach Inline or reference The [ForEach] function that will be called for every message, receiving arguments key of type K and value of type V . Example: from: input_stream via: - type: peek forEach: print_key_and_value to: output_stream reduce Combine the values of records in this stream by the grouped key. Records with null key or value are ignored. Combining implies that the type of the aggregate result is the same as the type of the input value, similar to aggregate(Initializer, Aggregator) . Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . KGroupedTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . adder Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . subtractor Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type V . SessionWindowedKStream <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type session . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type window . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Example: [ yaml ] ---- from: input_stream via: - type: groupBy mapper: my_mapper_function - type: reduce reducer: expression: value1+value2 - type: toStream to: output_stream repartition Materialize this stream to an auto-generated repartition topic with a given number of partitions, using a custom partitioner. Similar to auto-repartitioning, the topic will be created with infinite retention time and data will be automatically purged. The topic will be named as \"${applicationId}- -repartition\". Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> numberOfPartitions integer No The number of partitions of the repartitioned topic. partitioner Inline or reference No A custom [Partitioner] function to partition records. Example: from: input_stream via: - type: repartition name: my_partitioner numberOfPartitions: 3 partitioner: my_own_partitioner - type: peek forEach: print_key_and_value - type: toStream to: output_stream selectKey This operation takes a message and transforms the key into a new key, which may have a different type. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,V> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . Example: from: input_stream via: - type: selectKey mapper: expression: str(key) # convert key from source type to string to: output_stream suppress Suppress some updates from this changelog stream, determined by the supplied Suppressed configuration. When windowCloses is selected and no further restrictions are provided, then this is interpreted as Suppressed.untilWindowCloses(unbounded()) . Stream Type Returns Parameter Value Type Required Description KTable <K,V> KTable <K,V> until string Yes This value can either be timeLimit or windowCloses . Note that timeLimit suppression works on any stream, while windowCloses suppression works only on Windowed streams. For the latter, see [windowByTime] or [windowBySession]. duration string No The Duration to suppress updates (only when until == timeLimit ) maxBytes int No The maximum number of bytes to suppress updates maxRecords int No The maximum number of records to suppress updates bufferFullStrategy string No Can be one of emitEarlyWhenFull , shutdownWhenFull Example: from: input_table via: - type: suppress until: timeLimit duration: 30s maxBytes: 128000 maxRecords: 10000 bufferFullStrategy: emitEarlyWhenFull - type: peek forEach: print_key_and_value - type: toStream to: output_stream toStream Convert a KTable into a KStream object. Stream Type Returns Parameter Value Type Required Description KTable <K,V> KStream <KR,V> mapper Inline or reference No A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . If no mapper is provided, then keys remain unchanged. Example: from: input_table via: - type: toStream to: output_stream transformKey This is an alias for selectKey . Example: from: input_stream via: - type: transformKey mapper: expression: str(key) # convert key from source type to string to: output_stream transformKeyValue This is an alias for map . Example: from: input_stream via: - type: transformKeyValue mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream transformKeyValueToKeyValueList This is an alias for flatMap . Example: from: input_stream via: - type: transformKeyValueToKeyValueList mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream transformKeyValueToValueList This is an alias for flapMapValues . Example: from: input_stream via: - type: transformKeyValueToValueList mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream transformMetadata This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A [MetadataTransformer] function that converts the metadata (Kafka headers, timestamp) of every record in the stream. It gets a metadata object as input and should return the same type, but potentially with modified fields. Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream transformValue This is an alias for mapValues . Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream windowBySession Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> SessionWindowedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. [CogroupedKStream] <K,V> SessionWindowedCogroupedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBySession inactivityGap: 1h grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream windowByTime Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Description KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBy windowType: time duration: 1h advanceBy: 15m grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream Sink Operations as Pipelines closed of with as can be referred by other pipelines as their starting reference. This allows for a common part of processing logic to be placed in its own pipeline in KSML, serving as an intermediate result. Applies to Value Type Required Description Any pipeline <K,V> string Yes The name under which the pipeline result can be referenced by other pipelines. Example: pipelines: first: from: some_source_topic via: - type: ... as: first_pipeline second: from: first_pipeline via: - type: ... to: ... Here, the first pipeline ends by sending its output to a stream internally called first_pipeline . This stream is used as input for the second pipeline. branch Branches out messages from the input stream into several branches based on predicates. Each branch is defined as a list item below the branch operation. Branch predicates are defined using the if keyword. Messages are only processed by one of the branches, namely the first one for which the predicate returns true . Applies to Value Type Required Description KStream <K,V> List of branch definitions Yes See for description of branch definitions below. Branches in KSML are nested pipelines, which are parsed without the requirement of a source attribute. Each branch accepts the following parameters: Branch element Value Type Required Description if Inline Predicate or reference No The Predicate function that determines if the message is sent down this branch, or is passed on to the next branch in line. Inline All pipeline parameters, see [Pipeline] Yes The inlined pipeline describes the topology of the specific branch. Example: from: some_source_topic branch: - if: expression: value['color'] == 'blue' to: ksml_sensordata_blue - if: expression: value['color'] == 'red' to: ksml_sensordata_red - forEach: code: | print('Unknown color sensor: '+value[\"color\"]) In this example, the first two branches are entered if the respective predicate matches (the color attribute of value matches a certain color). If the predicate returns false , then the next predicate/branch is tried. Only the last branch in the list can be a sink operation. forEach This sends each message to a custom defined function. This function is expected to handle each message as its final step. The function does not (need to) return anything. Applies to Value Type Description KStream <K,V> Inline or reference The [ForEach] function that is called for every record on the source stream. Its arguments are key of type K and value of type V . Examples: forEach: my_foreach_function forEach: code: print(value) print This sends each message to a custom defined print function. This function is expected to handle each message as the final in the pipeline. The function does not (need to) return anything. As target, you can specify a filename. If none is specified, then all messages are printed to stdout. Applies to Parameter Value Type Required Description KStream <K,V> filename string No The filename to output records to. If nothing is specified, then messages will be printed on stdout. label string No A label to attach to every output record. mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value should be of type string and is sent to the specified file or stdout. Examples: from: source via: - type: ... print: filename: file.txt mapper: expression: \"record value: \" + str(value) to Messages are sent directly to a named Stream . Applies to Value Type Required Description KStream <K,V> Inline [Topic] or reference to a stream, table or global table Yes The name of a defined stream . Examples: to: my_target_topic from: source via: - type: ... to: topic: my_target_topic keyType: someType valueType: someOtherType partitioner: expression: hash_of(key) toTopicNameExtractor Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. This operation acts as a Sink and is always the last operation in a pipeline . Applies to Value Type Required Description KStream <K,V> Inline or reference Yes The [TopicNameExtractor] function that is called for every message and returns the topic name to which the message shall be written. Examples: toTopicNameExtractor: my_extractor_function toTopicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' elif key == 'sensor2': return 'ksml_sensordata_sensor2' else: return 'ksml_sensordata_sensor0'","title":"Operstions"},{"location":"reference-docs/operations/#operations","text":"","title":"Operations"},{"location":"reference-docs/operations/#table-of-contents","text":"Introduction Operations aggregate cogroup convertKey convertKeyValue convertValue count filter filterNot flatMap flatMapValues groupBy groupByKey join leftJoin map mapKey mapValue mapValues merge outerJoin peek reduce repartition selectKey suppress toStream transformKey transformKeyValue transformKeyValueToKeyValueList transformKeyValueToValueList transformValue windowBySession windowByTime Sink Operations as branch forEach print to toTopicNameExtractor","title":"Table of Contents"},{"location":"reference-docs/operations/#introduction","text":"Pipelines in KSML have a beginning, a middle and (optionally) an end. Operations form the middle part of pipelines. They are modeled as separate YAML entities, where each operation takes input from the previous operation and applies its own logic. The returned stream then serves as input for the next operation.","title":"Introduction"},{"location":"reference-docs/operations/#transform-operations","text":"Transformations are operations that take an input stream and convert it to an output stream. This section lists all supported transformations. Each one states the type of stream it returns. Parameter Value Type Description name string The name of the operation. Note that not all combinations of output/input streams are supported by Kafka Streams. The user that writes the KSML definition needs to make sure that streams that result from one operation can actually serve as input to the next. KSML does type checking and will exit with an error when operations that can not be chained together are listed after another in the KSML definition.","title":"Transform Operations"},{"location":"reference-docs/operations/#aggregate","text":"This operation aggregates multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . KGroupedTable <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . adder Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . subtractor Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type VR . SessionWindowedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> KTable <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . SessionWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type session . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . merger Inline or reference Yes A Merger function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the merged result, also of type V . TimeWindowedCogroupedKStream <K,V> KTable <Windowed<K>,VR> store Store configuration No An optional Store configuration, should be of type window . initializer Inline or reference Yes An Initializer function, which takes no arguments and returns a value of type VR . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream","title":"aggregate"},{"location":"reference-docs/operations/#cogroup","text":"This operation cogroups multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> [CogroupedKStream] <K,VR> aggregator Inline or reference Yes An Aggregator function, which takes a key of type K , a value of type V and aggregatedValue of type VR . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type VR . [CogroupedKStream] <K,V> n/a n/a n/a n/a This method is currently not supported in KSML. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: cogroup aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Note: this operation was added to KSML for completion purposes, but is not considered ready or fully functional. Feel free to experiment, but don't rely on this in production. Syntax changes may occur in future KSML releases.","title":"cogroup"},{"location":"reference-docs/operations/#convertkey","text":"This built-in operation takes a message and converts the key into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,V> into string The type to convert the key into. Conversion to KR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKey into: json to: output_stream","title":"convertKey"},{"location":"reference-docs/operations/#convertkeyvalue","text":"This built-in operation takes a message and converts the key and value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <KR,VR> into string The type to convert the key and value into. Conversion of key into KR and value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertKeyValue into: (json,xml) to: output_stream","title":"convertKeyValue"},{"location":"reference-docs/operations/#convertvalue","text":"This built-in operation takes a message and converts the value into a given type. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> into string The type to convert the value into. Conversion of value into VR is done by KSML. Example: from: topic: input_stream keyType: string valueType: string via: - type: convertValue into: xml to: output_stream","title":"convertValue"},{"location":"reference-docs/operations/#count","text":"This operation counts the number of messages and returns a table multiple values into a single one by repeatedly calling an aggregator function. It can operate on a range of stream types. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . KGroupedTable <K,V> KTable <K,Long> store Store configuration No An optional Store configuration, should be of type keyValue . SessionWindowedKStream <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type session . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,Long> store Store configuration No An optional Store configuration, should be of type window . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: count - type: toStream to: output_stream","title":"count"},{"location":"reference-docs/operations/#filter","text":"Filter all incoming messages according to some predicate. The predicate function is called for every message. Only when the predicate returns true , then the message will be sent to the output stream. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. KTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . if Yes Inline or reference A Predicate function, which returns True if the message can pass the filter, False otherwise. Example: from: input_stream via: - type: filter if: expression: key.startswith('a') to: output_stream","title":"filter"},{"location":"reference-docs/operations/#filternot","text":"This operation works exactly like filter , but negates all predicates before applying them. That means messages for which the predicate returns False are accepted, while those that the predicate returns True for are filtered out. See filter for details on how to implement.","title":"filterNot"},{"location":"reference-docs/operations/#flatmap","text":"This operation takes a message and transforms it into zero, one or more new messages, which may have different key and value types than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [(KR,VR)] containing a list of transformed key and value pairs. Example: from: input_stream via: - type: flatMap mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream","title":"flatMap"},{"location":"reference-docs/operations/#flatmapvalues","text":"This operation takes a message and transforms it into zero, one or more new values, which may have different value types than the source. Every entry in the result list is combined with the source key and produced on the output stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,VR> mapper Inline or reference A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a list of type [VR] containing a list of transformed value s. Example: from: input_stream via: - type: flatMapValues mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream","title":"flatMapValues"},{"location":"reference-docs/operations/#groupby","text":"Group the records of a stream by value resulting from a KeyValueMapper. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. KTable <K,V> KGroupedTable <KR,V> store Store configuration No An optional Store configuration, should be of type keyValue . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V and returns a value of type KR to group the stream by. Example: from: input_stream via: - type: groupBy mapper: expression: value[\"some_field\"] resultType: string - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream","title":"groupBy"},{"location":"reference-docs/operations/#groupbykey","text":"Group the records of a stream by the stream's key. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KGroupedStream <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . Example: from: input_stream via: - type: groupByKey - type: aggregate initializer: expression: 0 aggregator: expression: value1+value2 - type: toStream to: output_stream","title":"groupByKey"},{"location":"reference-docs/operations/#join","text":"Join records of this stream with another stream's records using inner join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: join stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream","title":"join"},{"location":"reference-docs/operations/#leftjoin","text":"Join records of this stream with another stream's records using left join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KStream <K,V> KStream <K,VR> globalTable string Yes The name of the global table to join with. The global table should be of key type GK and value type GV . mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of type GK of the records from the GlobalTable to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . KTable <K,V> KTable <K,VR> store Store configuration No The Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . foreignKeyExtractor Inline or reference No A [ForeignKeyExtractor] function, which takes a value of type V , which needs to be converted into the key type KO of the table to join with. valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . partitioner Inline or reference No A [Partitioner] function, which partitions the records on the primary stream. otherPartitioner Inline or reference No A [Partitioner] function, which partitions the records on the join table. Example: from: input_stream via: - type: leftJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream","title":"leftJoin"},{"location":"reference-docs/operations/#map","text":"This operation takes a message and transforms the key and value into a new key and value, which can each have a different type than the source message key and value. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a tuple of type (KR,VR) containing the transformed key and value . Example: from: input_stream via: - type: map mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream","title":"map"},{"location":"reference-docs/operations/#mapkey","text":"This is an alias for selectKey . Example: from: input_stream via: - type: mapKey mapper: expression: str(key) # convert key from source type to string to: output_stream","title":"mapKey"},{"location":"reference-docs/operations/#mapvalue","text":"This is an alias for mapValues . Example: from: input_stream via: - type: mapValue mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"mapValue"},{"location":"reference-docs/operations/#mapvalues","text":"This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return type should be a value of type VR . Example: from: input_stream via: - type: mapValues mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"mapValues"},{"location":"reference-docs/operations/#merge","text":"Merge this stream and the given stream into one larger stream. There is no ordering guarantee between records from this stream and records from the provided stream in the merged stream. Relative order is preserved within each input stream though (ie, records within one input stream are processed in order). Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> stream string The name of the stream to merge with. Example: from: input_stream via: - type: merge stream: second_stream to: output_stream","title":"merge"},{"location":"reference-docs/operations/#outerjoin","text":"Join records of this stream with another stream's records using outer join. The join is computed on the records' key with join predicate thisStream.key == otherStream.key . If both streams are not tables, then their timestamps need to be close enough as defined by timeDifference. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type window . stream string Yes The name of the stream to join with. The stream should be of key type K and value type VR . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a key of type K , and two values value1 and value2 of type V . The return value is the joined value of type VR . timeDifference duration Yes The maximum allowed between two joined records. grace duration No A grace period during with out-of-order to-be-joined records may still arrive. KTable <K,V> KStream <K,VR> store Store configuration No An optional Store configuration, should be of type keyValue . table string Yes The name of the table to join with. The table should be of key type K and value type VO . valueJoiner Inline or reference Yes A [ValueJoiner] function, which takes a value1 of type V from the source table and a value2 of type VO from the join table. The return value is the joined value of type VR . Example: from: input_stream via: - type: outerJoin stream: second_stream valueJoiner: my_key_value_mapper timeDifference: 1m to: output_stream","title":"outerJoin"},{"location":"reference-docs/operations/#peek","text":"Perform an action on each record of a stream. This is a stateless record-by-record operation. Peek is a non-terminal operation that triggers a side effect (such as logging or statistics collection) and returns an unchanged stream. Stream Type Returns Parameter Value Type Description KStream <K,V> KStream <K,V> forEach Inline or reference The [ForEach] function that will be called for every message, receiving arguments key of type K and value of type V . Example: from: input_stream via: - type: peek forEach: print_key_and_value to: output_stream","title":"peek"},{"location":"reference-docs/operations/#reduce","text":"Combine the values of records in this stream by the grouped key. Records with null key or value are ignored. Combining implies that the type of the aggregate result is the same as the type of the input value, similar to aggregate(Initializer, Aggregator) . Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . KGroupedTable <K,V> KTable <K,V> store Store configuration No An optional Store configuration, should be of type keyValue . adder Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . subtractor Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should remove the key/value from the previously calculated aggregateValue and return a new aggregate value of type V . SessionWindowedKStream <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type session . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . [TimeWindowedKStreamObject] <K,V> KTable <Windowed<K>,V> store Store configuration No An optional Store configuration, should be of type window . reducer Inline or reference Yes A Reducer function, which takes a key of type K , a value of type V and aggregatedValue of type V . It should add the key/value to the previously calculated aggregateValue and return a new aggregate value of type V . Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: aggregate initializer: expression: 0 aggregator: expression: aggregatedValue + value - type: toStream to: output_stream Example: [ yaml ] ---- from: input_stream via: - type: groupBy mapper: my_mapper_function - type: reduce reducer: expression: value1+value2 - type: toStream to: output_stream","title":"reduce"},{"location":"reference-docs/operations/#repartition","text":"Materialize this stream to an auto-generated repartition topic with a given number of partitions, using a custom partitioner. Similar to auto-repartitioning, the topic will be created with infinite retention time and data will be automatically purged. The topic will be named as \"${applicationId}- -repartition\". Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,V> numberOfPartitions integer No The number of partitions of the repartitioned topic. partitioner Inline or reference No A custom [Partitioner] function to partition records. Example: from: input_stream via: - type: repartition name: my_partitioner numberOfPartitions: 3 partitioner: my_own_partitioner - type: peek forEach: print_key_and_value - type: toStream to: output_stream","title":"repartition"},{"location":"reference-docs/operations/#selectkey","text":"This operation takes a message and transforms the key into a new key, which may have a different type. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <KR,V> mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . Example: from: input_stream via: - type: selectKey mapper: expression: str(key) # convert key from source type to string to: output_stream","title":"selectKey"},{"location":"reference-docs/operations/#suppress","text":"Suppress some updates from this changelog stream, determined by the supplied Suppressed configuration. When windowCloses is selected and no further restrictions are provided, then this is interpreted as Suppressed.untilWindowCloses(unbounded()) . Stream Type Returns Parameter Value Type Required Description KTable <K,V> KTable <K,V> until string Yes This value can either be timeLimit or windowCloses . Note that timeLimit suppression works on any stream, while windowCloses suppression works only on Windowed streams. For the latter, see [windowByTime] or [windowBySession]. duration string No The Duration to suppress updates (only when until == timeLimit ) maxBytes int No The maximum number of bytes to suppress updates maxRecords int No The maximum number of records to suppress updates bufferFullStrategy string No Can be one of emitEarlyWhenFull , shutdownWhenFull Example: from: input_table via: - type: suppress until: timeLimit duration: 30s maxBytes: 128000 maxRecords: 10000 bufferFullStrategy: emitEarlyWhenFull - type: peek forEach: print_key_and_value - type: toStream to: output_stream","title":"suppress"},{"location":"reference-docs/operations/#tostream","text":"Convert a KTable into a KStream object. Stream Type Returns Parameter Value Type Required Description KTable <K,V> KStream <KR,V> mapper Inline or reference No A KeyValueMapper function, which takes a key of type K and a value of type V . The return value is the key of resulting stream, which is of type KR . If no mapper is provided, then keys remain unchanged. Example: from: input_table via: - type: toStream to: output_stream","title":"toStream"},{"location":"reference-docs/operations/#transformkey","text":"This is an alias for selectKey . Example: from: input_stream via: - type: transformKey mapper: expression: str(key) # convert key from source type to string to: output_stream","title":"transformKey"},{"location":"reference-docs/operations/#transformkeyvalue","text":"This is an alias for map . Example: from: input_stream via: - type: transformKeyValue mapper: expression: (str(key), str(value)) # convert key and value from source type to string to: output_stream","title":"transformKeyValue"},{"location":"reference-docs/operations/#transformkeyvaluetokeyvaluelist","text":"This is an alias for flatMap . Example: from: input_stream via: - type: transformKeyValueToKeyValueList mapper: expression: [ (key,value), (key,value) ] # duplicate all incoming messages to: output_stream","title":"transformKeyValueToKeyValueList"},{"location":"reference-docs/operations/#transformkeyvaluetovaluelist","text":"This is an alias for flapMapValues . Example: from: input_stream via: - type: transformKeyValueToValueList mapper: expression: [ value+1, value+2, value+3 ] # creates 3 new messages [key,VR] for every input message to: output_stream","title":"transformKeyValueToValueList"},{"location":"reference-docs/operations/#transformmetadata","text":"This operation takes a message and transforms its value to a new value, which may have different value type than the source. Stream Type Returns Parameter Value Type Required Description KStream <K,V> KStream <K,VR> mapper Inline or reference Yes A [MetadataTransformer] function that converts the metadata (Kafka headers, timestamp) of every record in the stream. It gets a metadata object as input and should return the same type, but potentially with modified fields. Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"transformMetadata"},{"location":"reference-docs/operations/#transformvalue","text":"This is an alias for mapValues . Example: from: input_stream via: - type: transformValue mapper: expression: str(value) # convert value from source type to String to: output_stream","title":"transformValue"},{"location":"reference-docs/operations/#windowbysession","text":"Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Required Description KGroupedStream <K,V> SessionWindowedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. [CogroupedKStream] <K,V> SessionWindowedCogroupedKStream <K,V> inactivityGap Duration Yes The maximum inactivity gap with which keys are grouped. grace Duration No The grace duration allowing for out-of-order messages to still be associated with the right session. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBySession inactivityGap: 1h grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream","title":"windowBySession"},{"location":"reference-docs/operations/#windowbytime","text":"Create a new windowed KStream instance that can be used to perform windowed aggregations. For more details on the different types of windows, please refer to [WindowTypes]|[this page]. Stream Type Returns Parameter Value Type Description KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. KGroupedStream <K,V> TimeWindowedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value sliding . timeDifference Duration The time difference parameter for the [SlidingWindows] object. grace Duration (Optional) The grace parameter for the [SlidingWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value hopping . advanceBy Duration The amount by which each window is advanced. If this value is not specified, then it will be equal to duration , which gives tumbling windows. If you make this value smaller than duration you will get hopping windows. grace Duration (Optional) The grace parameter for the [TimeWindows] object. [CogroupedKStream] <K,V> TimeWindowedCogroupedKStream <K,V> windowType string Fixed value tumbling . duration Duration The duration parameter for the [TimeWindows] object. grace Duration (Optional) The grace parameter for the [TimeWindows] object. Example: from: input_stream via: - type: groupBy mapper: my_mapper_function - type: windowedBy windowType: time duration: 1h advanceBy: 15m grace: 5m - type: reduce reducer: my_reducer_function - type: toStream to: output_stream","title":"windowByTime"},{"location":"reference-docs/operations/#sink-operations","text":"","title":"Sink Operations"},{"location":"reference-docs/operations/#as","text":"Pipelines closed of with as can be referred by other pipelines as their starting reference. This allows for a common part of processing logic to be placed in its own pipeline in KSML, serving as an intermediate result. Applies to Value Type Required Description Any pipeline <K,V> string Yes The name under which the pipeline result can be referenced by other pipelines. Example: pipelines: first: from: some_source_topic via: - type: ... as: first_pipeline second: from: first_pipeline via: - type: ... to: ... Here, the first pipeline ends by sending its output to a stream internally called first_pipeline . This stream is used as input for the second pipeline.","title":"as"},{"location":"reference-docs/operations/#branch","text":"Branches out messages from the input stream into several branches based on predicates. Each branch is defined as a list item below the branch operation. Branch predicates are defined using the if keyword. Messages are only processed by one of the branches, namely the first one for which the predicate returns true . Applies to Value Type Required Description KStream <K,V> List of branch definitions Yes See for description of branch definitions below. Branches in KSML are nested pipelines, which are parsed without the requirement of a source attribute. Each branch accepts the following parameters: Branch element Value Type Required Description if Inline Predicate or reference No The Predicate function that determines if the message is sent down this branch, or is passed on to the next branch in line. Inline All pipeline parameters, see [Pipeline] Yes The inlined pipeline describes the topology of the specific branch. Example: from: some_source_topic branch: - if: expression: value['color'] == 'blue' to: ksml_sensordata_blue - if: expression: value['color'] == 'red' to: ksml_sensordata_red - forEach: code: | print('Unknown color sensor: '+value[\"color\"]) In this example, the first two branches are entered if the respective predicate matches (the color attribute of value matches a certain color). If the predicate returns false , then the next predicate/branch is tried. Only the last branch in the list can be a sink operation.","title":"branch"},{"location":"reference-docs/operations/#foreach","text":"This sends each message to a custom defined function. This function is expected to handle each message as its final step. The function does not (need to) return anything. Applies to Value Type Description KStream <K,V> Inline or reference The [ForEach] function that is called for every record on the source stream. Its arguments are key of type K and value of type V . Examples: forEach: my_foreach_function forEach: code: print(value)","title":"forEach"},{"location":"reference-docs/operations/#print","text":"This sends each message to a custom defined print function. This function is expected to handle each message as the final in the pipeline. The function does not (need to) return anything. As target, you can specify a filename. If none is specified, then all messages are printed to stdout. Applies to Parameter Value Type Required Description KStream <K,V> filename string No The filename to output records to. If nothing is specified, then messages will be printed on stdout. label string No A label to attach to every output record. mapper Inline or reference Yes A KeyValueMapper function, which takes a key of type K and a value of type V . The return value should be of type string and is sent to the specified file or stdout. Examples: from: source via: - type: ... print: filename: file.txt mapper: expression: \"record value: \" + str(value)","title":"print"},{"location":"reference-docs/operations/#to","text":"Messages are sent directly to a named Stream . Applies to Value Type Required Description KStream <K,V> Inline [Topic] or reference to a stream, table or global table Yes The name of a defined stream . Examples: to: my_target_topic from: source via: - type: ... to: topic: my_target_topic keyType: someType valueType: someOtherType partitioner: expression: hash_of(key)","title":"to"},{"location":"reference-docs/operations/#totopicnameextractor","text":"Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. This operation acts as a Sink and is always the last operation in a pipeline . Applies to Value Type Required Description KStream <K,V> Inline or reference Yes The [TopicNameExtractor] function that is called for every message and returns the topic name to which the message shall be written. Examples: toTopicNameExtractor: my_extractor_function toTopicNameExtractor: code: | if key == 'sensor1': return 'ksml_sensordata_sensor1' elif key == 'sensor2': return 'ksml_sensordata_sensor2' else: return 'ksml_sensordata_sensor0'","title":"toTopicNameExtractor"},{"location":"reference-docs/pipelines/","text":"Pipeline Table of Contents Introduction Definition Elements Source Operations Sink Introduction Pipelines form the heart of KSML streams logic. They take one or more input streams and apply processing logic to them. Output is passed on from operation to operation. Definition Pipelines are contained in the pipelines section. Each pipeline has a name, which is the name of the YAML tag. As an example, the following defines a pipeline called copy_pipeline , which consumes messages from some_input_stream and outputs the same messages to some_output_stream . pipelines: copy_pipeline: from: some_input_stream to: some_output_stream Elements All pipelines contain three elements: Source The source of a pipeline is marked with the from keyword. The value of the YAML node is the name of a defined Stream , Table or GlobalTable . See Streams for more information. Operations Once a source was selected, a list of operations can be applied to the input. The list is started through the keyword via , below which a list of operations is defined. Example: pipelines: copy_pipeline: from: some_input_stream via: - type: peek forEach: my_peek_function - type: transformKeyValue mapper: my_transform_function to: some_output_stream Each operation must have a type field, which indicates the type of operations applied to the input. See Operations for a full list of operations that can be applied. Sinks After all transformation operations are applied, a pipeline can define a sink to which all messages are sent. There are four sink types in KSML: Sink type Description as Allows the pipeline result to be saved under an internal name, which can later be referenced. Pipelines defined after this point may refer to this name in their from statement. branch This statement allows the pipeline to be split up in several branches. Each branch filters messages with an if statement. Messages will be processed only by the first branch of which the if statement is true. forEach Sends every message to a function, without expecting any return type. Because there is no return type, the pipeline always stops after this statement. print Prints out every message according to a given output specification. to Sends all output messages to a specific target. This target can be a pre-defined stream , table or globalTable , or an inline-defined topic. toTopicNameExtractor Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. For more information, see the respective documentation on pipeline definitions in the definitions section of the KSML language spec . Duration Some pipeline operations require specifying durations. Durations can be expressed as strings with the following syntax: ###x where # is a positive number between 0 and 999999 and x is an optional letter from the following table: Letter Description none Duration in milliseconds s Duration in seconds m Duration in minutes h Duration in hours d Duration in days w Duration in weeks Examples: 100 ==> hundred milliseconds 30s ==> thirty seconds 8h ==> eight hours 2w ==> two weeks Note that durations are not a data type that can used as key or value on a Kafka topic.","title":"Pipelines"},{"location":"reference-docs/pipelines/#pipeline","text":"","title":"Pipeline"},{"location":"reference-docs/pipelines/#table-of-contents","text":"Introduction Definition Elements Source Operations Sink","title":"Table of Contents"},{"location":"reference-docs/pipelines/#introduction","text":"Pipelines form the heart of KSML streams logic. They take one or more input streams and apply processing logic to them. Output is passed on from operation to operation.","title":"Introduction"},{"location":"reference-docs/pipelines/#definition","text":"Pipelines are contained in the pipelines section. Each pipeline has a name, which is the name of the YAML tag. As an example, the following defines a pipeline called copy_pipeline , which consumes messages from some_input_stream and outputs the same messages to some_output_stream . pipelines: copy_pipeline: from: some_input_stream to: some_output_stream","title":"Definition"},{"location":"reference-docs/pipelines/#elements","text":"All pipelines contain three elements:","title":"Elements"},{"location":"reference-docs/pipelines/#source","text":"The source of a pipeline is marked with the from keyword. The value of the YAML node is the name of a defined Stream , Table or GlobalTable . See Streams for more information.","title":"Source"},{"location":"reference-docs/pipelines/#operations","text":"Once a source was selected, a list of operations can be applied to the input. The list is started through the keyword via , below which a list of operations is defined. Example: pipelines: copy_pipeline: from: some_input_stream via: - type: peek forEach: my_peek_function - type: transformKeyValue mapper: my_transform_function to: some_output_stream Each operation must have a type field, which indicates the type of operations applied to the input. See Operations for a full list of operations that can be applied.","title":"Operations"},{"location":"reference-docs/pipelines/#sinks","text":"After all transformation operations are applied, a pipeline can define a sink to which all messages are sent. There are four sink types in KSML: Sink type Description as Allows the pipeline result to be saved under an internal name, which can later be referenced. Pipelines defined after this point may refer to this name in their from statement. branch This statement allows the pipeline to be split up in several branches. Each branch filters messages with an if statement. Messages will be processed only by the first branch of which the if statement is true. forEach Sends every message to a function, without expecting any return type. Because there is no return type, the pipeline always stops after this statement. print Prints out every message according to a given output specification. to Sends all output messages to a specific target. This target can be a pre-defined stream , table or globalTable , or an inline-defined topic. toTopicNameExtractor Messages are passed onto a user function, which returns the name of the topic that message needs to be sent to. For more information, see the respective documentation on pipeline definitions in the definitions section of the KSML language spec .","title":"Sinks"},{"location":"reference-docs/pipelines/#duration","text":"Some pipeline operations require specifying durations. Durations can be expressed as strings with the following syntax: ###x where # is a positive number between 0 and 999999 and x is an optional letter from the following table: Letter Description none Duration in milliseconds s Duration in seconds m Duration in minutes h Duration in hours d Duration in days w Duration in weeks Examples: 100 ==> hundred milliseconds 30s ==> thirty seconds 8h ==> eight hours 2w ==> two weeks Note that durations are not a data type that can used as key or value on a Kafka topic.","title":"Duration"},{"location":"reference-docs/runners/","text":"KSML Runner Table of Contents Introduction Configuration Namespace support Starting a container Introduction KSML is built around the ksml module, which provides the core functionality for parsing KSML definition files and converting them into Kafka Streams topologies. To keep KSML lightweight and flexible, it does not execute these topologies directly. Instead, execution is handled by the ksml-runner module \u2014 a standalone Java application that loads KSML configurations and runs the corresponding Kafka Streams applications. The ksml-runner supports standard Kafka configurations and includes advanced features for working with Kafka clusters that use namespacing (e.g., tenant-aware deployments). Example runner configurations are provided below. Configuration The configuration file passed to the KSML runner is in YAML format and should contain at least the following: ksml: applicationServer: # The application server is currently only offering REST querying of state stores enabled: true # true if you want to enable REST querying of state stores host: 0.0.0.0 # by default listen on all interfaces port: 8080 # port to listen on configDirectory: /ksml/config # Location of the KSML definitions. Default is the current working directory schemaDirectory: /ksml/schemas # Location of the schema definitions. Default is the config directory storageDirectory: /ksml/data # Where the stateful data is written. Defaults is the default JVM temp directory errorHandling: # how to handle errors consume: log: true # log errors logPayload: true # log message payloads upon error loggerName: ConsumeError # logger name handler: continueOnFail # continue or stop on error process: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProcessError # logger name handler: stopOnFail # continue or stop on error produce: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProduceError # logger name handler: continueOnFail # continue or stop on error enableProducers: true # False to disable producers in the KSML definition enablePipelines: true # False to disable pipelines in the KSML definition definitions: # KSML definition files from the working directory namedDefinition1: definition1.yaml namedDefinition2: definition2.yaml namedDefinition3: <more here...> kafka: # Kafka streams configuration options application.id: io.ksml.example.processor bootstrap.servers: broker-1:9092,broker-2:9092 security.protocol: SSL ssl.protocol: TLSv1.3 ssl.enabled.protocols: TLSv1.3,TLSv1.2 ssl.endpoint.identification.algorithm: \"\" ssl.truststore.location: /ksml/config/truststore.jks ssl.truststore.password: password-for-truststore # Schema Registry client configuration, needed when schema registry is used schema.registry.url: http://schema-registry:8083 schema.registry.ssl.truststore.location: /ksml/config/truststore.jks schema.registry.ssl.truststore.password: password-for-truststore Using with Axual platform or other namespaced Kafka clusters A special mode for connecting to clusters that use namespaced Kafka resources is available. This mode can be activated by specifying the namespace pattern to use. This pattern will be resolved to a complete name by KSML using the provided configuration options. The following config will resolve the backing topic of a stream or table kafka: # The patterns for topics, groups and transactional ids. # Each field between the curly braces must be specified in the configuration, except the topic, # group.id and transactional.id fields, which is used to identify the place where the resource name # is used topic.pattern: \"{tenant}-{instance}-{environment}-{topic}\" group.id.pattern: \"{tenant}-{instance}-{environment}-{group.id}\" transactional.id.pattern: \"{tenant}-{instance}-{environment}-{transactional.id}\" # Additional configuration options used for resolving the pattern to values tenant: \"ksmldemo\" instance: \"dta\" environment: \"dev\" Starting a container To start a container the KSML definitions and Runner configuration files need to be available in a directory mounted inside the docker container. The default Runner configuration filename is ksml-runner.yaml . If no arguments are given, the runner will look for this file in the home directory ## -w sets the current working directory in the container docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot ## or docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot /ksml/ksml-runner.yaml or, if the runner configuration is in a different file, like my-runner.yaml . docker run --rm -ti -v /path/to/local/ksml/directory:/ksml axual/ksml-axual:latest /ksml/my-runner.yaml","title":"Runners"},{"location":"reference-docs/runners/#ksml-runner","text":"","title":"KSML Runner"},{"location":"reference-docs/runners/#table-of-contents","text":"Introduction Configuration Namespace support Starting a container","title":"Table of Contents"},{"location":"reference-docs/runners/#introduction","text":"KSML is built around the ksml module, which provides the core functionality for parsing KSML definition files and converting them into Kafka Streams topologies. To keep KSML lightweight and flexible, it does not execute these topologies directly. Instead, execution is handled by the ksml-runner module \u2014 a standalone Java application that loads KSML configurations and runs the corresponding Kafka Streams applications. The ksml-runner supports standard Kafka configurations and includes advanced features for working with Kafka clusters that use namespacing (e.g., tenant-aware deployments). Example runner configurations are provided below.","title":"Introduction"},{"location":"reference-docs/runners/#configuration","text":"The configuration file passed to the KSML runner is in YAML format and should contain at least the following: ksml: applicationServer: # The application server is currently only offering REST querying of state stores enabled: true # true if you want to enable REST querying of state stores host: 0.0.0.0 # by default listen on all interfaces port: 8080 # port to listen on configDirectory: /ksml/config # Location of the KSML definitions. Default is the current working directory schemaDirectory: /ksml/schemas # Location of the schema definitions. Default is the config directory storageDirectory: /ksml/data # Where the stateful data is written. Defaults is the default JVM temp directory errorHandling: # how to handle errors consume: log: true # log errors logPayload: true # log message payloads upon error loggerName: ConsumeError # logger name handler: continueOnFail # continue or stop on error process: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProcessError # logger name handler: stopOnFail # continue or stop on error produce: log: true # log errors logPayload: true # log message payloads upon error loggerName: ProduceError # logger name handler: continueOnFail # continue or stop on error enableProducers: true # False to disable producers in the KSML definition enablePipelines: true # False to disable pipelines in the KSML definition definitions: # KSML definition files from the working directory namedDefinition1: definition1.yaml namedDefinition2: definition2.yaml namedDefinition3: <more here...> kafka: # Kafka streams configuration options application.id: io.ksml.example.processor bootstrap.servers: broker-1:9092,broker-2:9092 security.protocol: SSL ssl.protocol: TLSv1.3 ssl.enabled.protocols: TLSv1.3,TLSv1.2 ssl.endpoint.identification.algorithm: \"\" ssl.truststore.location: /ksml/config/truststore.jks ssl.truststore.password: password-for-truststore # Schema Registry client configuration, needed when schema registry is used schema.registry.url: http://schema-registry:8083 schema.registry.ssl.truststore.location: /ksml/config/truststore.jks schema.registry.ssl.truststore.password: password-for-truststore","title":"Configuration"},{"location":"reference-docs/runners/#using-with-axual-platform-or-other-namespaced-kafka-clusters","text":"A special mode for connecting to clusters that use namespaced Kafka resources is available. This mode can be activated by specifying the namespace pattern to use. This pattern will be resolved to a complete name by KSML using the provided configuration options. The following config will resolve the backing topic of a stream or table kafka: # The patterns for topics, groups and transactional ids. # Each field between the curly braces must be specified in the configuration, except the topic, # group.id and transactional.id fields, which is used to identify the place where the resource name # is used topic.pattern: \"{tenant}-{instance}-{environment}-{topic}\" group.id.pattern: \"{tenant}-{instance}-{environment}-{group.id}\" transactional.id.pattern: \"{tenant}-{instance}-{environment}-{transactional.id}\" # Additional configuration options used for resolving the pattern to values tenant: \"ksmldemo\" instance: \"dta\" environment: \"dev\"","title":"Using with Axual platform or other namespaced Kafka clusters"},{"location":"reference-docs/runners/#starting-a-container","text":"To start a container the KSML definitions and Runner configuration files need to be available in a directory mounted inside the docker container. The default Runner configuration filename is ksml-runner.yaml . If no arguments are given, the runner will look for this file in the home directory ## -w sets the current working directory in the container docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot ## or docker run --rm -ti -v /path/to/local/ksml/directory:/ksml -w /ksml axual/ksml-axual:snapshot /ksml/ksml-runner.yaml or, if the runner configuration is in a different file, like my-runner.yaml . docker run --rm -ti -v /path/to/local/ksml/directory:/ksml axual/ksml-axual:latest /ksml/my-runner.yaml","title":"Starting a container"},{"location":"reference-docs/stores/","text":"State Stores Introduction Several stream operations use state stores to retain (intermediate) results of their calculations. State stores are typically configured with some additional parameters to limit their data storage (retention time) and/or determine when data is emitted from them to the next stream operation. stores: owner_count_store: name: owner_count retention: 3m caching: false Configuration State store configurations are defined by the following tags: Parameter Value Type Default Required Description name string none Optional The name of the state store. This field is not mandatory, but operations that use the state store configuration will require a name for their store. If the store configuration does not specify an explicit name, then the operation will default back to the operation's name, specified with its name attribute. If that name is unspecified, then an exception will be thrown. In general, it is considered good practice to always specify the store name explicitly with its definition. type string none Required The type of the state store. Possible types are keyValue , session and window . persistent boolean false Optional true if the state store should be retained on disk. See [link] for more information on how Kafka Streams maintains state store state in a state directory. When this parameter is false or undefined, the state store is (re)built up in memory during upon KSML start. timestamped boolean false Optional (Only relevant for keyValue and window stores) true if all messages in the state store need to be timestamped. This effectively changes the state store from type to . The timestamp contains the last timestamp that updated the aggregated value in the window. versioned boolean false Optional (Only relevant for keyValue stores) true if elements in the store are versioned, false otherwise keyType string none Required The key type of the state store. See Types for more information. valueType string none Required The value type of the state store. See Types for more information. caching boolean false Optional This parameter controls the internal state store caching. When true , the state store caches entries and does not emit every state change but only. When false all changes to the state store will be emitted immediately. logging boolean false Optional This parameter determines whether state changes are written out to a changelog topic, or not. When true all state store changes are produced to a changelog topic. The changelog topic is named appId-storeName-changelog . When false no changelog topic is written to. Example: stores: owner_count_store: name: owner_count retention: 3m caching: false pipelines: main: from: sensor_source via: - type: groupBy name: ksml_sensordata_grouped mapper: expression: value[\"owner\"] resultType: string - type: windowedBy windowType: time duration: 20s grace: 40s - type: aggregate store: owner_count_store # refer to predefined store configuration above initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long Instead of referring to predefined state store configurations, you may also use an inline definition for the store: - type: aggregate store: name: owner_count retention: 3m caching: false initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long","title":"Stores"},{"location":"reference-docs/stores/#state-stores","text":"","title":"State Stores"},{"location":"reference-docs/stores/#introduction","text":"Several stream operations use state stores to retain (intermediate) results of their calculations. State stores are typically configured with some additional parameters to limit their data storage (retention time) and/or determine when data is emitted from them to the next stream operation. stores: owner_count_store: name: owner_count retention: 3m caching: false","title":"Introduction"},{"location":"reference-docs/stores/#configuration","text":"State store configurations are defined by the following tags: Parameter Value Type Default Required Description name string none Optional The name of the state store. This field is not mandatory, but operations that use the state store configuration will require a name for their store. If the store configuration does not specify an explicit name, then the operation will default back to the operation's name, specified with its name attribute. If that name is unspecified, then an exception will be thrown. In general, it is considered good practice to always specify the store name explicitly with its definition. type string none Required The type of the state store. Possible types are keyValue , session and window . persistent boolean false Optional true if the state store should be retained on disk. See [link] for more information on how Kafka Streams maintains state store state in a state directory. When this parameter is false or undefined, the state store is (re)built up in memory during upon KSML start. timestamped boolean false Optional (Only relevant for keyValue and window stores) true if all messages in the state store need to be timestamped. This effectively changes the state store from type to . The timestamp contains the last timestamp that updated the aggregated value in the window. versioned boolean false Optional (Only relevant for keyValue stores) true if elements in the store are versioned, false otherwise keyType string none Required The key type of the state store. See Types for more information. valueType string none Required The value type of the state store. See Types for more information. caching boolean false Optional This parameter controls the internal state store caching. When true , the state store caches entries and does not emit every state change but only. When false all changes to the state store will be emitted immediately. logging boolean false Optional This parameter determines whether state changes are written out to a changelog topic, or not. When true all state store changes are produced to a changelog topic. The changelog topic is named appId-storeName-changelog . When false no changelog topic is written to. Example: stores: owner_count_store: name: owner_count retention: 3m caching: false pipelines: main: from: sensor_source via: - type: groupBy name: ksml_sensordata_grouped mapper: expression: value[\"owner\"] resultType: string - type: windowedBy windowType: time duration: 20s grace: 40s - type: aggregate store: owner_count_store # refer to predefined store configuration above initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long Instead of referring to predefined state store configurations, you may also use an inline definition for the store: - type: aggregate store: name: owner_count retention: 3m caching: false initializer: expression: 0 resultType: long aggregator: expression: aggregatedValue+1 resultType: long","title":"Configuration"},{"location":"reference-docs/streams/","text":"Streams Table of Contents Introduction Stream Table GlobalTable Introduction Every KSML definition file contains a list of declared streams. There are three types of streams supported: Type Kafka Streams equivalent Description Stream KStream KStream is an abstraction of a record stream of KeyValue pairs, i.e., each record is an independent entity/event in the real world. For example a user X might buy two items I1 and I2, and thus there might be two records , in the stream. A KStream is either defined from one or multiple Kafka topics that are consumed message by message or the result of a KStream transformation. A KTable can also be converted into a KStream. A KStream can be transformed record by record, joined with another KStream, KTable, GlobalKTable, or can be aggregated into a KTable. Table KTable KTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. A KTable is either defined from a single Kafka topic that is consumed message by message or the result of a KTable transformation. An aggregation of a KStream also yields a KTable. A KTable can be transformed record by record, joined with another KTable or KStream, or can be re-partitioned and aggregated into a new KTable. GlobalTable GlobalKTable GlobalKTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. GlobalKTable can only be used as right-hand side input for stream-table joins. In contrast to a KTable that is partitioned over all KafkaStreams instances, a GlobalKTable is fully replicated per KafkaStreams instance. Every partition of the underlying topic is consumed by each GlobalKTable, such that the full set of data is available in every KafkaStreams instance. This provides the ability to perform joins with KStream without having to repartition the input stream. All joins with the GlobalKTable require that a KeyValueMapper is provided that can map from the KeyValue of the left hand side KStream to the key of the right hand side GlobalKTable. The definitions of these stream types are done as described below. Stream Example: streams: my_stream_reference: topic: some_kafka_topic keyType: string valueType: string offsetResetPolicy: earliest timestampExtractor: my_timestamp_extractor Table Example: tables: my_table_reference: topic: some_kafka_topic keyType: string valueType: string store: <keyValue state store reference or inline definition> GlobalTable Example: globalTables: my_global_table_reference: topic: some_kafka_topic keyType: string valueType: string","title":"Streams"},{"location":"reference-docs/streams/#streams","text":"","title":"Streams"},{"location":"reference-docs/streams/#table-of-contents","text":"Introduction Stream Table GlobalTable","title":"Table of Contents"},{"location":"reference-docs/streams/#introduction","text":"Every KSML definition file contains a list of declared streams. There are three types of streams supported: Type Kafka Streams equivalent Description Stream KStream KStream is an abstraction of a record stream of KeyValue pairs, i.e., each record is an independent entity/event in the real world. For example a user X might buy two items I1 and I2, and thus there might be two records , in the stream. A KStream is either defined from one or multiple Kafka topics that are consumed message by message or the result of a KStream transformation. A KTable can also be converted into a KStream. A KStream can be transformed record by record, joined with another KStream, KTable, GlobalKTable, or can be aggregated into a KTable. Table KTable KTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. A KTable is either defined from a single Kafka topic that is consumed message by message or the result of a KTable transformation. An aggregation of a KStream also yields a KTable. A KTable can be transformed record by record, joined with another KTable or KStream, or can be re-partitioned and aggregated into a new KTable. GlobalTable GlobalKTable GlobalKTable is an abstraction of a changelog stream from a primary-keyed table. Each record in this changelog stream is an update on the primary-keyed table with the record key as the primary key. GlobalKTable can only be used as right-hand side input for stream-table joins. In contrast to a KTable that is partitioned over all KafkaStreams instances, a GlobalKTable is fully replicated per KafkaStreams instance. Every partition of the underlying topic is consumed by each GlobalKTable, such that the full set of data is available in every KafkaStreams instance. This provides the ability to perform joins with KStream without having to repartition the input stream. All joins with the GlobalKTable require that a KeyValueMapper is provided that can map from the KeyValue of the left hand side KStream to the key of the right hand side GlobalKTable. The definitions of these stream types are done as described below.","title":"Introduction"},{"location":"reference-docs/streams/#stream","text":"Example: streams: my_stream_reference: topic: some_kafka_topic keyType: string valueType: string offsetResetPolicy: earliest timestampExtractor: my_timestamp_extractor","title":"Stream"},{"location":"reference-docs/streams/#table","text":"Example: tables: my_table_reference: topic: some_kafka_topic keyType: string valueType: string store: <keyValue state store reference or inline definition>","title":"Table"},{"location":"reference-docs/streams/#globaltable","text":"Example: globalTables: my_global_table_reference: topic: some_kafka_topic keyType: string valueType: string","title":"GlobalTable"},{"location":"reference-docs/types/","text":"Types Table of Contents Introduction Notations Primitives Any Duration Enum List Struct Tuple Windowed Introduction KSML supports a wide range of simple and complex types. In addition, each data type can be read from Kafka, and written to Kafka, using Notations . More about Notations in the following paragraph. The paragraphs after will dive more into the data types that KSML supports. Notations KSML has a number of simple and complex data types. Simple data types contain a single value. For example, a long or a string type can hold only one value. Complex data types contain multiple values. For example, a list or a struct can contain zero or more other simple or complex values. For example, a struct is comparable to a key/value map, where all keys are of type string . Its contents could be represented as: { \"name\": \"Albert\", \"lastName\": \"Einstein\", \"dateOfBirth\": \"14-08-1879\", \"profession\": \"Patent clerk\", \"children\": 3, \"isAlive\": false } There are several ways in which structured objects are read from or written to Kafka topics. For instance, using AVRO, JSON or XML. Each use the same struct but translate it differently to a binary or string representation that can be stored in Kafka. To facilitate these different ways of writing, KSML supports Notations . Each notation can be seen as 'the format that data is written in' to Kafka. Below is a list of all supported notations: Notation Link Implementations AVRO https://avro.apache.org apicurio_avro, confluent_avro CSV https://en.wikipedia.org/wiki/Comma-separated_values JSON https://json.org SOAP https://en.wikipedia.org/wiki/SOAP XML https://en.wikipedia.org/wiki/XML Notations are a natural extension to data types. They can be used as: notation:datatype For example: avro:SensorData # AVRO with schema SensorData (loaded from SensorData.avsc) json # Schemaless JSON xml:PersonSchema # XML with schema PersonSchema (loaded from PersonSchema.xsd) Primitives The following native types are supported. Type Description ?, or any Any type null, or none Null type, available for variables without a value (eg. Kafka tombstone messages, or optional AVRO fields) boolean Boolean values, ie. true or false double Double precision floating point float Single precision floating point byte 8-bit integer short 16-bit integer int 32-bit integer long 64-bit long bytes Byte array string String of characters struct Key-value map, where with string keys and values of any type Any The special type ? or any can be used in places where input is uncertain. Code that deals with input of this type should always perform proper type checking before assuming any specific underlying type. Duration Some fields in the KSML spec are of type duration . These fields have a fixed format 123x , where 123 is an integer and x is any of the following: : milliseconds s : seconds m : minutes h : hours d : days w : weeks Enum Enumerations can be defined as individual types, through: enum(literal1, literal2, ...) List Lists contain elements of the same type. They are defined using: [elementType] Examples: [string] [long] [avro:SensorData] [(long,string)] Struct Structs are key-value maps, as usually found in AVRO or JSON messages. They are defined using: struct Tuple Tuples combine multiple subtypes into one. For example (1, \"text\") is a tuple containing an integer and a string element. Tuple types always have a fixed number of elements. Examples: (long, string) (avro:SensorData, string, long, string) ([string], long) Union Unions are 'either-or' types. They have their own internal structure and can be described by respective data schema. Unions are defined using: union(type1, type2, ...) Examples: union(null, string) union(avro:SensorData, long, string) Windowed Some Kafka Streams operations modify the key type from K to Windowed\\ . Kafka Streams uses the Windowed\\ type to group Kafka messages with similar keys together. The result is always a time-bound window, with a defined start and end time. KSML can convert the internal Windowed\\ type into a struct type with five fields: start : The window start timestamp (type long ) end : The window end timestamp (type long ) startTime : The window start time (type string ) endTime : The window end time (type string ) key : The key used to group items together in this window (type is the same as the original key type) However, in pipelines or topic definitions users may need to refer to this type explicitly. This is done in the following manner: notation:windowed(keytype) For example: avro:windowed(avro:SensorData) xml:windowed(long)","title":"Types"},{"location":"reference-docs/types/#types","text":"","title":"Types"},{"location":"reference-docs/types/#table-of-contents","text":"Introduction Notations Primitives Any Duration Enum List Struct Tuple Windowed","title":"Table of Contents"},{"location":"reference-docs/types/#introduction","text":"KSML supports a wide range of simple and complex types. In addition, each data type can be read from Kafka, and written to Kafka, using Notations . More about Notations in the following paragraph. The paragraphs after will dive more into the data types that KSML supports.","title":"Introduction"},{"location":"reference-docs/types/#notations","text":"KSML has a number of simple and complex data types. Simple data types contain a single value. For example, a long or a string type can hold only one value. Complex data types contain multiple values. For example, a list or a struct can contain zero or more other simple or complex values. For example, a struct is comparable to a key/value map, where all keys are of type string . Its contents could be represented as: { \"name\": \"Albert\", \"lastName\": \"Einstein\", \"dateOfBirth\": \"14-08-1879\", \"profession\": \"Patent clerk\", \"children\": 3, \"isAlive\": false } There are several ways in which structured objects are read from or written to Kafka topics. For instance, using AVRO, JSON or XML. Each use the same struct but translate it differently to a binary or string representation that can be stored in Kafka. To facilitate these different ways of writing, KSML supports Notations . Each notation can be seen as 'the format that data is written in' to Kafka. Below is a list of all supported notations: Notation Link Implementations AVRO https://avro.apache.org apicurio_avro, confluent_avro CSV https://en.wikipedia.org/wiki/Comma-separated_values JSON https://json.org SOAP https://en.wikipedia.org/wiki/SOAP XML https://en.wikipedia.org/wiki/XML Notations are a natural extension to data types. They can be used as: notation:datatype For example: avro:SensorData # AVRO with schema SensorData (loaded from SensorData.avsc) json # Schemaless JSON xml:PersonSchema # XML with schema PersonSchema (loaded from PersonSchema.xsd)","title":"Notations"},{"location":"reference-docs/types/#primitives","text":"The following native types are supported. Type Description ?, or any Any type null, or none Null type, available for variables without a value (eg. Kafka tombstone messages, or optional AVRO fields) boolean Boolean values, ie. true or false double Double precision floating point float Single precision floating point byte 8-bit integer short 16-bit integer int 32-bit integer long 64-bit long bytes Byte array string String of characters struct Key-value map, where with string keys and values of any type","title":"Primitives"},{"location":"reference-docs/types/#any","text":"The special type ? or any can be used in places where input is uncertain. Code that deals with input of this type should always perform proper type checking before assuming any specific underlying type.","title":"Any"},{"location":"reference-docs/types/#duration","text":"Some fields in the KSML spec are of type duration . These fields have a fixed format 123x , where 123 is an integer and x is any of the following: : milliseconds s : seconds m : minutes h : hours d : days w : weeks","title":"Duration"},{"location":"reference-docs/types/#enum","text":"Enumerations can be defined as individual types, through: enum(literal1, literal2, ...)","title":"Enum"},{"location":"reference-docs/types/#list","text":"Lists contain elements of the same type. They are defined using: [elementType] Examples: [string] [long] [avro:SensorData] [(long,string)]","title":"List"},{"location":"reference-docs/types/#struct","text":"Structs are key-value maps, as usually found in AVRO or JSON messages. They are defined using: struct","title":"Struct"},{"location":"reference-docs/types/#tuple","text":"Tuples combine multiple subtypes into one. For example (1, \"text\") is a tuple containing an integer and a string element. Tuple types always have a fixed number of elements. Examples: (long, string) (avro:SensorData, string, long, string) ([string], long)","title":"Tuple"},{"location":"reference-docs/types/#union","text":"Unions are 'either-or' types. They have their own internal structure and can be described by respective data schema. Unions are defined using: union(type1, type2, ...) Examples: union(null, string) union(avro:SensorData, long, string)","title":"Union"},{"location":"reference-docs/types/#windowed","text":"Some Kafka Streams operations modify the key type from K to Windowed\\ . Kafka Streams uses the Windowed\\ type to group Kafka messages with similar keys together. The result is always a time-bound window, with a defined start and end time. KSML can convert the internal Windowed\\ type into a struct type with five fields: start : The window start timestamp (type long ) end : The window end timestamp (type long ) startTime : The window start time (type string ) endTime : The window end time (type string ) key : The key used to group items together in this window (type is the same as the original key type) However, in pipelines or topic definitions users may need to refer to this type explicitly. This is done in the following manner: notation:windowed(keytype) For example: avro:windowed(avro:SensorData) xml:windowed(long)","title":"Windowed"}]}